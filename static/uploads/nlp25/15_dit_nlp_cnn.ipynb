{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw5jACJmuLSQ"
      },
      "source": [
        "# DIT Natural Language Processing Lesson 2025\n",
        "\n",
        "## CNNs on text\n",
        "\n",
        "This notebook starts to run apart from the originally publishe in NLP in Action. The reason is that the original implementation requires way too many resources and, as a result, crashes in the freely available colab servers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim"
      ],
      "metadata": {
        "id": "s0txMavemiCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaEklyOTuLSU"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "import tarfile\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "from keras.preprocessing import sequence   # necessary for padding\n",
        "from keras.models import Sequential        # Base Keras NN model\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D # Convolution layer and pooling\n",
        "from keras.layers import Dense, Dropout, Activation # The objects for each layer\n",
        "from keras.layers import Input\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from psutil import virtual_memory\n",
        "from random import shuffle\n",
        "from urllib import request\n",
        "from tqdm.auto import tqdm  # to show a smart progress meter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To check what is the amount of memory available\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "vCW9uqJ8ADMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bejGp6_8uLST"
      },
      "source": [
        "For this exercise, we first download Stanford's [Large Movie Review Dataset](https://ai.stanford.edu/%7eamaas/data/sentiment/aclImdb_v1.tar.gz). More information on the corpus at [Learning Word Vectors for Sentiment Analysis](https://ai.stanford.edu/%7eamaas/papers/wvSent_acl2011.pdf). We need pre-trained embeddings as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tp25FdWuLSU"
      },
      "outputs": [],
      "source": [
        "PATH_TO_CORPUS = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "CORPUS_FILE_NAME = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "PATH_TO_GOOGLENEWS_VECTORS =\"https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\"\n",
        "GOOGLE_VECTORS = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "CORPUS_PATH = \"aclImdb/train\"\n",
        "\n",
        "def download_file(url_to_file, path_to_file):\n",
        "  if os.path.isfile(path_to_file):\n",
        "    print(\"A local copy of the file exists already:\", path_to_file, \"\\nDoing nothing\")\n",
        "  else:\n",
        "    request.urlretrieve(url_to_file, path_to_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the embeddings\n",
        "\n",
        "download_file(PATH_TO_GOOGLENEWS_VECTORS, GOOGLE_VECTORS)"
      ],
      "metadata": {
        "id": "gmtdGr1mwt5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and untaring the corpus\n",
        "\n",
        "download_file(PATH_TO_CORPUS, CORPUS_FILE_NAME)\n",
        "with tarfile.open(CORPUS_FILE_NAME) as f:\n",
        "  f.extractall(path=\".\")"
      ],
      "metadata": {
        "id": "dPsvTLPNwqso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS9GTF46uLSV"
      },
      "outputs": [],
      "source": [
        "# A method to read and shuffle all instances (one per file).\n",
        "# Positive (negative) instances are in the pos (neg) folder\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    This is dependent on your training data source but we will\n",
        "    try to generalize it as best as possible.\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "\n",
        "    # glob.glob returns a list of path names that match pathname\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "    shuffle(dataset)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j933LPb3uLSV"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the data\n",
        "dataset = pre_process_data(CORPUS_PATH)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHPh0YJ_uLSW"
      },
      "outputs": [],
      "source": [
        "# Loading the word2vec embeddings\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format(GOOGLE_VECTORS,\n",
        "    binary=True, limit=400000)\n",
        "\n",
        "# If you want to use less memory (e.g., you are just playing around), you could\n",
        "# reduce the size of the vocabulary\n",
        "# (e.g., to 200000 or to 100000)\n",
        "# word_vectors = KeyedVectors.load_word2vec_format(GOOGLE_VECTORS,\n",
        "#     binary=True, limit=200000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCftM2k0uLSX"
      },
      "outputs": [],
      "source": [
        "# Method to tokenise and vectorise the data\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    # expcted = [] this line appears in the book, but it's not necessary here!\n",
        "    for sample in tqdm(dataset):\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(word_vectors[token])\n",
        "            except KeyError:\n",
        "                pass # No matching token in the Google w2v vocab\n",
        "        np_vec = np.vstack(sample_vecs)\n",
        "        vectorized_data.append(np_vec)\n",
        "    return vectorized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmNj4h6RuLSX"
      },
      "outputs": [],
      "source": [
        "# Method to get the target labels\n",
        "def collect_expected(dataset):\n",
        "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7ZdwNotuLSY"
      },
      "outputs": [],
      "source": [
        "# Vectorising the dataset and extracting the gold standard\n",
        "x = tokenize_and_vectorize(dataset)\n",
        "y = collect_expected(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEamyzAcuLSY"
      },
      "outputs": [],
      "source": [
        "# Creating training and validation partitions\n",
        "# n_samples = 0\n",
        "# if n_samples > 0:\n",
        "#     x = x[:n_samples]\n",
        "#     y = y[:n_samples]\n",
        "\n",
        "split_point = int(len(x)*.8)\n",
        "\n",
        "# Original alternative\n",
        "x_train = x[:split_point] # there's a typo in this line, if copying from the book\n",
        "y_train = y[:split_point]\n",
        "x_test = x[split_point:]\n",
        "y_test = y[split_point:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d37C-neQuLSY"
      },
      "outputs": [],
      "source": [
        "# Network parameters\n",
        "\n",
        "maxlen = 400          # maximum length of the text (why?)\n",
        "batch_size = 32       # number of samples before backpropagating\n",
        "embedding_dims = 300  # Same as Google's\n",
        "filters = 250         # (!)\n",
        "kernel_size = 3       # remember: filter=kernel (we have a scalar this time)\n",
        "hidden_dims = 250     # number of neurons in the final layer\n",
        "epochs = 2            # number of training epochs\n",
        "\n",
        "# If you want to try to run with less memory, an alternative is to reduce\n",
        "# the maximum length of the input and the size of the network\n",
        "# maxlen = 100          # maximum length of the text (why?)\n",
        "# batch_size = 32       # number of samples before backpropagating\n",
        "# embedding_dims = 300  # Same as Google's\n",
        "# filters = 120\n",
        "# kernel_size = 3       # remember: filter=kernel (we have a scalar this time)\n",
        "# hidden_dims = 150     # number of neurons in the final layer\n",
        "# epochs = 2            # number of training epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCAfR68luLSa"
      },
      "source": [
        "**Back to the slides** to see what is \"padding\" in text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2jG5_z2uLSa"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtfKT5fcuLSa"
      },
      "outputs": [],
      "source": [
        "class Collator:\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 batch_size,\n",
        "                 ) -> None:\n",
        "        self.maxlen = maxlen\n",
        "        self.batch_size = batch_size\n",
        "        pass\n",
        "\n",
        "    def padding_and_truncating(self, x, y):\n",
        "        \"\"\"\n",
        "        Add zeros at the end of the representation for short instances,\n",
        "        truncate longer ones to the maxlen\n",
        "        \"\"\"\n",
        "        vec_dim = len(x[0][0])\n",
        "        N = len(x)\n",
        "        X = np.zeros((N, self.maxlen, vec_dim))  # preallocate a np array\n",
        "        Y = np.array(y)\n",
        "\n",
        "        for i, tokens in enumerate(x):\n",
        "            length = min(len(tokens), self.maxlen)\n",
        "            if length > 0:\n",
        "                X[i, :length] = np.asarray(tokens[:length])  # fill the np array\n",
        "        return X, Y\n",
        "\n",
        "    def collate(self, X, Y, N, epochs = 1):\n",
        "        \"\"\"\n",
        "        This method is used to feed batches into the `model.fit` method\n",
        "        \"\"\"\n",
        "        for _ in range(epochs):\n",
        "            '''\n",
        "            This `for _ in range(epochs):` loop is here because\n",
        "            the `for` below needs to be able to be called `epochs` times\n",
        "            so each time the `for` is called the iterator is replenished\n",
        "            for the new epoch\n",
        "            '''\n",
        "            for start in range(0, N, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                x_batch, y_batch = self.padding_and_truncating(\n",
        "                    X[start:end],\n",
        "                    Y[start:end]\n",
        "                    )\n",
        "                yield x_batch, y_batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collator = Collator(maxlen = maxlen, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "lQO4qusWBWKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One nice thing is that since we're using Keras (which is based on TensorFlow) then we need to pad each batch of the dataset to the same length, otherwise Keras will rebuild the whole computation graph for the backpropagation for each batch, making training very (very) slow. So if instead you keep each sequence in the batch to the same `maxlen` then it goes very fast."
      ],
      "metadata": {
        "id": "0pFgltkGp_We"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUdU26LIuLSa"
      },
      "source": [
        "## Building the network\n",
        "\n",
        "Adding the convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FwkDLbEuLSa"
      },
      "outputs": [],
      "source": [
        "print('Building model...')\n",
        "model = Sequential()   # The standard NN model\n",
        "model.add(\n",
        "    Input(\n",
        "        shape=(maxlen, embedding_dims)\n",
        "        )\n",
        "    )\n",
        "model.add(Conv1D(      # Adding a convolutional layer\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        padding='valid',   # in this example the output is going to be slightly smaller\n",
        "        activation='relu',\n",
        "        strides=1,         # the shift\n",
        "        )\n",
        "    )\n",
        "# Formulation: max (0, dot(filter, 3-gram))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWCmkuuZuLSa"
      },
      "outputs": [],
      "source": [
        "# Adding the max pooling\n",
        "# Alternatives\n",
        "# - GlobalMaxPooling1D() (the max for the entire filter's output)\n",
        "# - MaxPooling1D(n)  (the max for a specific area of n; default n=2)\n",
        "# - AvgPooling1D(n)\n",
        "\n",
        "model.add(GlobalMaxPooling1D())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDQdsfsZuLSa"
      },
      "source": [
        "**back to the slides to see what is pooling (and drop out)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGb5P7BluLSa"
      },
      "outputs": [],
      "source": [
        "# Adding dropout (20% of the data will be \"cancelled\")\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIeYhPXuuLSb"
      },
      "outputs": [],
      "source": [
        "# Adding the classification layer\n",
        "# sigmoid range: [0,1]\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988ejk35uLSb"
      },
      "source": [
        "**back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compile the network, using binary cross entropy as the loss function and Adam as the optimiser. Visit [this website for further details on this loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a/) and [this optimiser](https://optimization.cbe.cornell.edu/index.php?title=Adam)."
      ],
      "metadata": {
        "id": "th0ZrW22B-EC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWwi37nnuLSb"
      },
      "outputs": [],
      "source": [
        "# Compiling the CNN\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za2KQuoyuLSb"
      },
      "outputs": [],
      "source": [
        "# Here we use the collator, which is in charge of iterating over the dataset\n",
        "# for each batch, rather than just loading the whole dataset.\n",
        "model.fit(\n",
        "    collator.collate(x_train, y_train, N = len(x_train), epochs=epochs),\n",
        "    steps_per_epoch=len(x_train) // batch_size,\n",
        "    validation_data=collator.collate(x_test, y_test, N = len(x_test), epochs=epochs),\n",
        "    validation_steps=len(x_test) // batch_size,\n",
        "    epochs=epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTyXu8veuLSc"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "model_structure = model.to_json()\n",
        "with open(\"cnn_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)  # saves just the architecture\n",
        "model.save_weights(\"cnn.weights.h5\")  # saves the weights\n",
        "# You can run fit many times on the same model (it will continue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zhZfNxOuLSc"
      },
      "outputs": [],
      "source": [
        "# (Re)loading the model (not necessary here, but anyway)\n",
        "from keras.models import model_from_json\n",
        "with open(\"cnn_model.json\", \"r\") as json_file:\n",
        "    json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('cnn.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e6ggldtuLSc"
      },
      "outputs": [],
      "source": [
        "# Predicting a new instance\n",
        "\n",
        "# Notice we have both positive and negative words here\n",
        "sample_negative = \"\"\"I hate that the dismal weather had me down for so long,\n",
        "when will it break! Ugh, when does happiness return? The sun is\n",
        "blinding and the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
        "# Super positive sample\n",
        "sample_positive = \"\"\"I love that incredible weather!\n",
        "I feel like this happiness will last forever!\n",
        "The sun is super nice and the breeze is soothing.\n",
        "I could stay like this forever.\n",
        "\"\"\"\n",
        "\n",
        "# The first value is a \"fake\" class (this is the expected input)\n",
        "data_dummy = [(0, sample_negative), (0, sample_positive)]\n",
        "x_dummy = tokenize_and_vectorize(data_dummy)\n",
        "y_dummy = collect_expected(data_dummy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model logits (logits = raw model output)\n",
        "x_dummy_padded, y_dummy = collator.padding_and_truncating(x_dummy, y_dummy)\n",
        "preds = model.predict(x_dummy_padded)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "cMSU4g-Ir0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSCE6q5GuLSd"
      },
      "outputs": [],
      "source": [
        "# Get the class by thresholding the logits\n",
        "(preds > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnSmL3eNuLSd"
      },
      "source": [
        "**End of the notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}