{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CodgW4tN_iV0"
      },
      "source": [
        "# DIT Natural Language Processing lesson 2025\n",
        "\n",
        "## Using character-level representations for classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NyOEJmW_iV1"
      },
      "outputs": [],
      "source": [
        "# Importing the dependencies\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, Flatten, LSTM\n",
        "\n",
        "from random import shuffle\n",
        "from tqdm.auto import tqdm\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONSTANTS\n",
        "\n",
        "PATH_TO_CORPUS = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "CORPUS_FILE_NAME = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "CORPUS_PATH = \"aclImdb/train\"\n",
        "\n",
        "VALID = set([x for x in 'abcdefghijklmnopqrstuvwxyz0123456789\"\\'?!.,:; '])\n",
        "VALID"
      ],
      "metadata": {
        "id": "XLWR-v6dDqHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oBjEjoc_iV2"
      },
      "outputs": [],
      "source": [
        "# Download and set up the corpus\n",
        "\n",
        "def download_file(url_to_file, path_to_file):\n",
        "  if os.path.isfile(path_to_file):\n",
        "    print(\"A local copy of the file exists already:\", path_to_file, \"\\nDoing nothing\")\n",
        "  else:\n",
        "    request.urlretrieve(url_to_file, path_to_file)\n",
        "\n",
        "# Downloading and untaring the corpus\n",
        "\n",
        "download_file(PATH_TO_CORPUS, CORPUS_FILE_NAME)\n",
        "with tarfile.open(CORPUS_FILE_NAME) as f:\n",
        "  f.extractall(path=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BRESJI0_iV2"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    Load pos and neg examples from separate dirs then shuffle them\n",
        "    together.\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "      with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "    shuffle(dataset)\n",
        "    return dataset\n",
        "\n",
        "def collect_expected(dataset):\n",
        "    \"\"\"Extracting the expected output for all the instances\"\"\"\n",
        "    return [sample[0] for sample in dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl2SFpg7_iV2"
      },
      "outputs": [],
      "source": [
        "# Loading instances and expected classes (as usual)\n",
        "dataset = pre_process_data(CORPUS_PATH)\n",
        "expected = collect_expected(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bQv3ZjG_iV2"
      },
      "outputs": [],
      "source": [
        "def avg_len(data):\n",
        "    \"\"\"Computes the average length of the data\"\"\"\n",
        "    total_len = 0\n",
        "    for sample in data:\n",
        "        total_len += len(sample[1])\n",
        "    return total_len/len(data)\n",
        "avg_len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT9twxO6_iV2"
      },
      "source": [
        "The average word length is 202.44 (**Homework:** don't believe me and go find yourself).\n",
        "\n",
        "That is, we would unroll the network **6.5x**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_1rCzZT_iV2"
      },
      "outputs": [],
      "source": [
        "def clean_data(data):\n",
        "    \"\"\"Casefold, replace unknowns with UNK, and listify \"\"\"\n",
        "    new_data = []\n",
        "    # VALID = 'abcdefghijklmnopqrstuvwxyz0123456789\"\\'?!.,:; '\n",
        "    for sample in data:\n",
        "        new_sample = []\n",
        "        for char in sample[1].lower():  # Just grab the string, not the label\n",
        "            # Not extremely efficient procedure\n",
        "            if char in VALID:\n",
        "                new_sample.append(char)\n",
        "            else:\n",
        "                new_sample.append('UNK')\n",
        "\n",
        "        new_data.append(new_sample)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16ND9j3_iV2"
      },
      "source": [
        "**Homework**: turn the process to determine if a character is VALID more efficient\n",
        "\n",
        "(I did already, by switching VALID into a set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jujxxFkb_iV2"
      },
      "outputs": [],
      "source": [
        "# We are pading and truncating outside of the Collator this\n",
        "# time to set the data up in advance. As a result, the\n",
        "# Collator wont pad/truncate. It will simply feed the data\n",
        "# to fit by batches.\n",
        "\n",
        "def char_pad_trunc(data, maxlen):\n",
        "    \"\"\" We truncate to maxlen or add PAD tokens \"\"\"\n",
        "    new_dataset = []\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            new_data = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            pads = maxlen - len(sample)\n",
        "            new_data = sample + ['PAD'] * pads\n",
        "        else:\n",
        "            new_data = sample\n",
        "        new_dataset.append(new_data)\n",
        "    return new_dataset\n",
        "\n",
        "# Padding and truncating the input is not strictly necessary for RNNs; we\n",
        "# due to the recursion strategy we are applying here and to have a fair\n",
        "# comparison against the CNN (same input length)\n",
        "class Collator:\n",
        "  def __init__(self,\n",
        "                # maxlen,\n",
        "                batch_size,\n",
        "                ) -> None:\n",
        "    # self.maxlen = maxlen\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  # Since this time I am padding/truncating beforehand,\n",
        "  # I do not need to do it again\n",
        "  # def padding_and_truncating(self, x, y):\n",
        "  #   \"\"\"\n",
        "  #   Add zeros at the end of the representation for short instances,\n",
        "  #   truncate longer ones to the maxlen\n",
        "  #   \"\"\"\n",
        "  #   # vec_dim = len(x[0][0])\n",
        "  #   # N = len(x)\n",
        "  #   # X = np.zeros((N, self.maxlen, vec_dim))  # preallocate a np array\n",
        "  #   # Y = np.array(y)\n",
        "\n",
        "  #   # for i, tokens in enumerate(x):\n",
        "  #   #   length = min(len(tokens), self.maxlen)\n",
        "  #   #   if length > 0:\n",
        "  #   #     X[i, :length] = np.asarray(tokens[:length])  # fill the np array\n",
        "  #   # return X, Y\n",
        "  #   return x, y\n",
        "\n",
        "  def collate(self, X, Y, N, epochs = 1):\n",
        "    \"\"\"\n",
        "    This method is used to feed batches into the `model.fit` method\n",
        "    \"\"\"\n",
        "    for _ in range(epochs):\n",
        "      \"\"\"\n",
        "      This `for _ in range(epochs):` loop is here because\n",
        "      the `for` below needs to be able to be called `epochs` times\n",
        "      so each time the `for` is called the iterator is replenished\n",
        "      for the new epoch\n",
        "      \"\"\"\n",
        "      for start in range(0, N, self.batch_size):\n",
        "        end = start + self.batch_size\n",
        "        x_batch = X[start:end]\n",
        "        y_batch = Y[start:end]\n",
        "        # x_batch, y_batch = self.padding_and_truncating(\n",
        "        #   X[start:end],\n",
        "        #   Y[start:end]\n",
        "        #   )\n",
        "        yield x_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 800\n",
        "# maxlen = 1500\n",
        "batch_size = 32\n",
        "\n",
        "collator = Collator(\n",
        "    # maxlen = maxlen,\n",
        "    batch_size = batch_size)"
      ],
      "metadata": {
        "id": "X34TtYEZ3TLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrIZEDzD_iV3"
      },
      "outputs": [],
      "source": [
        "# Producing the one-hot encodings (no embeddings here)\n",
        "def create_dicts(data):\n",
        "    \"\"\" Modified from Keras LSTM example\"\"\"\n",
        "    chars = set()\n",
        "    for sample in data:\n",
        "        chars.update(set(sample))\n",
        "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "    return char_indices, indices_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_Zc0FEB_iV3"
      },
      "outputs": [],
      "source": [
        "def onehot_encode(dataset, char_indices, maxlen):\n",
        "    \"\"\"\n",
        "    One hot encode the tokens\n",
        "\n",
        "    Args:\n",
        "        dataset  list of lists of tokens\n",
        "        char_indices  dictionary of {key=character, value=index to use encoding vector}\n",
        "        maxlen  int  length of each sample\n",
        "    Return:\n",
        "        np array of shape (samples, tokens, encoding length)\n",
        "    \"\"\"\n",
        "    # print(len(dataset), maxlen)\n",
        "    X = np.zeros((len(dataset), maxlen, len(char_indices.keys())))\n",
        "    for i, sentence in enumerate(dataset):\n",
        "        # print(i)\n",
        "        for t, char in enumerate(sentence):\n",
        "            X[i, t, char_indices[char]] = 1\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLxho_XY_iV3"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the data\n",
        "\n",
        "listified_data = clean_data(dataset)\n",
        "\n",
        "\n",
        "common_length_data = char_pad_trunc(listified_data, maxlen)\n",
        "char_indices, indices_char = create_dicts(common_length_data)\n",
        "encoded_data = onehot_encode(common_length_data, char_indices, maxlen)\n",
        "\n",
        "# char_indices, indices_char = create_dicts(listified_data)\n",
        "# encoded_data = onehot_encode(listified_data, char_indices, maxlen)\n",
        "print(char_indices)\n",
        "print(indices_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4oHJhHF_iV3"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "split_point = int(len(encoded_data)*.8)\n",
        "\n",
        "x_train = encoded_data[:split_point]\n",
        "y_train = np.array(expected[:split_point])\n",
        "x_test = encoded_data[split_point:]\n",
        "y_test = np.array(expected[split_point:])\n",
        "# Pay attention: in the book they forgot to turn y_[train|test] into numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRryjhOs_iV3"
      },
      "outputs": [],
      "source": [
        "# A quick view to the first instance\n",
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCLGi2pu_iV3"
      },
      "outputs": [],
      "source": [
        "# Shape of the resulting array\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3exvHjzd_iV3"
      },
      "outputs": [],
      "source": [
        "# How many instances do we have?\n",
        "len(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B09OluV_iV3"
      },
      "source": [
        "**Q: What is  the size of the vocabulary?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oaas6TXz_iV3"
      },
      "outputs": [],
      "source": [
        "# Building the network\n",
        "\n",
        "# Embedding,\n",
        "num_neurons = 40\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input([maxlen, len(char_indices.keys())]))\n",
        "\n",
        "model.add(LSTM(\n",
        "    num_neurons,\n",
        "    return_sequences=True\n",
        "    # ,\n",
        "    # input_shape=(maxlen, len(char_indices.keys()))\n",
        "    )\n",
        "    )\n",
        "\n",
        "model.add(Dropout(.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYh-8OhW_iV4"
      },
      "source": [
        "[rmsprop](https://keras.io/api/optimizers/rmsprop/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EotClY2l_iV4"
      },
      "outputs": [],
      "source": [
        "# Training the network\n",
        "# batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Original fit (withput Collator-based batching)\n",
        "# model.fit(x_train, y_train,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     validation_data=(x_test, y_test)\n",
        "#     )\n",
        "\n",
        "model.fit(\n",
        "    collator.collate(x_train, y_train, N = len(x_train), epochs=epochs),\n",
        "    steps_per_epoch=len(x_train) // batch_size,\n",
        "    validation_data=collator.collate(x_test, y_test, N = len(x_test), epochs=epochs),\n",
        "    validation_steps=len(x_test) // batch_size,\n",
        "    epochs=epochs,\n",
        "    )\n",
        "# This would take between 5 and 10 minutes per epoch, depending on the hardware!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1LD_1Is_iV4"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "model_structure = model.to_json()\n",
        "with open(\"char_lstm_model3.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)\n",
        "model.save_weights(\"char_lstm.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqZaq9I5_iV4"
      },
      "source": [
        "Back to the slides"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}