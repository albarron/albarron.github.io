{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyPfhWMJ98hO"
      },
      "source": [
        "# DIT Natural Language Processing lesson 2025\n",
        "\n",
        "## Recurrent neural networks\n",
        "\n",
        "In this lesson, we build a recurrent neural network (RNN) to treat text as a sequence of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qyk8UeFGSNs"
      },
      "outputs": [],
      "source": [
        "! pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdFiAkHY98hR"
      },
      "outputs": [],
      "source": [
        "# Importing the dependencies\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input, Flatten, SimpleRNN\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from random import shuffle\n",
        "from tqdm.auto import tqdm  # to show a smart progress meter\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi53NVmT-YPh"
      },
      "outputs": [],
      "source": [
        "PATH_TO_CORPUS = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "CORPUS_FILE_NAME = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "PATH_TO_GOOGLENEWS_VECTORS =\"https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\"\n",
        "GOOGLE_VECTORS = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "CORPUS_PATH = \"aclImdb/train\"\n",
        "\n",
        "def download_file(url_to_file, path_to_file):\n",
        "  if os.path.isfile(path_to_file):\n",
        "    print(\"A local copy of the file exists already:\", path_to_file, \"\\nDoing nothing\")\n",
        "  else:\n",
        "    request.urlretrieve(url_to_file, path_to_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCh5TzxQ98hS"
      },
      "outputs": [],
      "source": [
        "# Downloading the embeddings\n",
        "\n",
        "download_file(PATH_TO_GOOGLENEWS_VECTORS, GOOGLE_VECTORS)\n",
        "\n",
        "# Downloading and untaring the corpus\n",
        "\n",
        "download_file(PATH_TO_CORPUS, CORPUS_FILE_NAME)\n",
        "with tarfile.open(CORPUS_FILE_NAME) as f:\n",
        "  f.extractall(path=\".\")\n",
        "\n",
        "# # Add the paths to the corpus. It should end in aclImdb/train\n",
        "# CORPUS_PATH = \"aclImdb/train\"\n",
        "# # Add the path to the embeddings. It should end in GoogleNews-vectors-negative300.bin.gz\n",
        "# GOOGLE_VECTORS = \"GoogleNews-vectors-negative300.bin.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBvErCFD98hS"
      },
      "source": [
        "**Note**: I am using the same methods as in the previous session.\n",
        "I could simply store them all in a .py file and import them, as\n",
        "with the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XtFutf798hV"
      },
      "outputs": [],
      "source": [
        "# Loading the embeddings\n",
        "word_vectors = KeyedVectors.load_word2vec_format(GOOGLE_VECTORS,\n",
        "    binary=True, limit=400000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfdAkqlc98hT"
      },
      "outputs": [],
      "source": [
        "# Data preprocessor\n",
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    Load pos and neg examples from separate dirs then shuffle them\n",
        "    together.\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "    shuffle(dataset)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiIV-dv498hT"
      },
      "outputs": [],
      "source": [
        "# Tokenizing and vectorizing all the instances\n",
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    for sample in tqdm(dataset):\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(word_vectors[token])\n",
        "            except KeyError:\n",
        "                pass\n",
        "        vectorized_data.append(sample_vecs)\n",
        "    return vectorized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO3ZyNaH98hV"
      },
      "outputs": [],
      "source": [
        "# Extracting the expected output for all the instances\n",
        "def collect_expected(dataset):\n",
        "  \"\"\" Peel off the target values from the dataset \"\"\"\n",
        "  return [sample[0] for sample in dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5m3L1tf98hW"
      },
      "outputs": [],
      "source": [
        "# Data preparation\n",
        "dataset = pre_process_data(CORPUS_PATH)\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)\n",
        "\n",
        "# Define training and validation data (even if it's called test here)\n",
        "split_point = int(len(vectorized_data) * .8)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1kQucNi98hX"
      },
      "outputs": [],
      "source": [
        "# Network parameters\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 300\n",
        "epochs = 2\n",
        "num_neurons = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oanGri0e98hU"
      },
      "outputs": [],
      "source": [
        "# Padding and truncating the input is not strictly necessary for RNNs; we\n",
        "# due to the recursion strategy we are applying here and to have a fair\n",
        "# comparison against the CNN (same input length)\n",
        "class Collator:\n",
        "  def __init__(self,\n",
        "                maxlen,\n",
        "                batch_size,\n",
        "                ) -> None:\n",
        "    self.maxlen = maxlen\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def padding_and_truncating(self, x, y):\n",
        "    \"\"\"\n",
        "    Add zeros at the end of the representation for short instances,\n",
        "    truncate longer ones to the maxlen\n",
        "    \"\"\"\n",
        "    vec_dim = len(x[0][0])\n",
        "    N = len(x)\n",
        "    X = np.zeros((N, self.maxlen, vec_dim))  # preallocate a np array\n",
        "    Y = np.array(y)\n",
        "\n",
        "    for i, tokens in enumerate(x):\n",
        "      length = min(len(tokens), self.maxlen)\n",
        "      if length > 0:\n",
        "        X[i, :length] = np.asarray(tokens[:length])  # fill the np array\n",
        "    return X, Y\n",
        "\n",
        "  def collate(self, X, Y, N, epochs = 1):\n",
        "    \"\"\"\n",
        "    This method is used to feed batches into the `model.fit` method\n",
        "    \"\"\"\n",
        "    for _ in range(epochs):\n",
        "      \"\"\"\n",
        "      This `for _ in range(epochs):` loop is here because\n",
        "      the `for` below needs to be able to be called `epochs` times\n",
        "      so each time the `for` is called the iterator is replenished\n",
        "      for the new epoch\n",
        "      \"\"\"\n",
        "      for start in range(0, N, self.batch_size):\n",
        "        end = start + self.batch_size\n",
        "        x_batch, y_batch = self.padding_and_truncating(\n",
        "          X[start:end],\n",
        "          Y[start:end]\n",
        "          )\n",
        "        yield x_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL-_K6osHqrT"
      },
      "outputs": [],
      "source": [
        "collator = Collator(maxlen = maxlen, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvOLbI2P98hY"
      },
      "outputs": [],
      "source": [
        "# Initializing the (empty) network\n",
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y7xJBP498hY"
      },
      "outputs": [],
      "source": [
        "# Adding one recurrent layer\n",
        "\n",
        "# In previous versions of keras (and in the book), the input\n",
        "# shape was defined as an argument to SimpleRNN. That way\n",
        "# still works, but adding an Input instead is adviced\n",
        "model.add(Input([maxlen, embedding_dims]))\n",
        "model.add(SimpleRNN(\n",
        "    num_neurons,\n",
        "    return_sequences=True,\n",
        "    # input_shape=(maxlen, embedding_dims)\n",
        "    )\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework: experiment with return_sequences=False and compare the results"
      ],
      "metadata": {
        "id": "r_vta8ht0VUi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4awyaMi98hY"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CQutbg298hY"
      },
      "outputs": [],
      "source": [
        "# Adding a dropout layer (remember why?)\n",
        "model.add(Dropout(.2))\n",
        "\n",
        "# Adding a flattening layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Adding the classifier\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E0z1eEW98hZ"
      },
      "source": [
        "**Flatten?** back to the slides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnqYvG8G98hZ"
      },
      "outputs": [],
      "source": [
        "# Compiling the network\n",
        "model.compile('rmsprop',\n",
        "              'binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 37,551 parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0tz5fKR98hZ"
      },
      "outputs": [],
      "source": [
        "# Training the network\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     validation_data=(x_test, y_test))\n",
        "\n",
        "model.fit(\n",
        "    collator.collate(x_train, y_train, N = len(x_train), epochs=epochs),\n",
        "    steps_per_epoch=len(x_train) // batch_size,\n",
        "    validation_data=collator.collate(x_test, y_test, N = len(x_test), epochs=epochs),\n",
        "    validation_steps=len(x_test) // batch_size,\n",
        "    epochs=epochs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JremPMdi98hZ"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEywD6dW98ha"
      },
      "outputs": [],
      "source": [
        "# Building a bigger network\n",
        "num_neurons = 100\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(\n",
        "    num_neurons,\n",
        "    return_sequences=True,\n",
        "    input_shape=(maxlen, embedding_dims))\n",
        "     )\n",
        "model.add(Dropout(.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfYUwUud98ha"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "# model.fit(x_train,\n",
        "#     y_train,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     validation_data=(x_test, y_test)\n",
        "#      )\n",
        "model.fit(\n",
        "    collator.collate(x_train, y_train, N = len(x_train), epochs=epochs),\n",
        "    steps_per_epoch=len(x_train) // batch_size,\n",
        "    validation_data=collator.collate(x_test, y_test, N = len(x_test), epochs=epochs),\n",
        "    validation_steps=len(x_test) // batch_size,\n",
        "    epochs=epochs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_v6JFmE98ha"
      },
      "source": [
        "The improvement is tiny*$\\rightarrow$ perhaps the network is too complex.\n",
        "\n",
        "\\* (depending on the random initialisation, it could even be worst!)\n",
        "\n",
        "Homework: try with 25 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsI-vz9M98ha"
      },
      "outputs": [],
      "source": [
        "# Saving the network\n",
        "model_structure = model.to_json()\n",
        "with open(\"simplernn_model2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)\n",
        "model.save_weights(\"simplernn2.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSR7qWSKSmH"
      },
      "source": [
        "Predicting on new instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDr_g85998hb"
      },
      "outputs": [],
      "source": [
        "# Notice we have both positive and negative words here\n",
        "sample_negative = \"\"\"I hate that the dismal weather had me down for so long,\n",
        "when will it break! Ugh, when does happiness return? The sun is\n",
        "blinding and the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
        "# Super positive sample\n",
        "sample_positive = \"\"\"I love that incredible weather!\n",
        "I feel like this happiness will last forever!\n",
        "The sun is super nice and the breeze is soothing.\n",
        "I could stay like this forever.\n",
        "\"\"\"\n",
        "\n",
        "# The first value is a \"fake\" class (this is the expected input)\n",
        "data_dummy = [(0, sample_negative), (0, sample_positive)]\n",
        "x_dummy = tokenize_and_vectorize(data_dummy)\n",
        "y_dummy = collect_expected(data_dummy)\n",
        "\n",
        "\n",
        "# model.predict(test_vec)\n",
        "x_dummy_padded, y_dummy = collator.padding_and_truncating(x_dummy, y_dummy)\n",
        "preds = model.predict(x_dummy_padded)\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZdd29MEKnQa"
      },
      "outputs": [],
      "source": [
        "# Get the class by thresholding the logits\n",
        "(preds > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHbzWyiM98hb"
      },
      "source": [
        "**End of the notebook**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YlInbbs98hb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}