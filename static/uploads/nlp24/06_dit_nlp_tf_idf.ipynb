{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpyhEiXHa1S1"
      },
      "source": [
        "# From bag-of-words to term frequency-inverse document frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnuyV2Nia1S2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "# from matplotlib import pyplot as plt\n",
        "# from nlpia.loaders import clean_columns\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import copy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2V_cVlta1S3"
      },
      "outputs": [],
      "source": [
        "# you (might) need to install nlpia again\n",
        "# We are not going to use this dependency as of now\n",
        "# ! pip3 install nlpia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvKHc0I7a1S3"
      },
      "source": [
        "## The difference between a binary and a _counting_ BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWKsIxvVa1S3"
      },
      "outputs": [],
      "source": [
        "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(sentence.lower())\n",
        "\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzcvLAISa1S4"
      },
      "outputs": [],
      "source": [
        "# Getting the vocabulary\n",
        "set(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzsrjfKua1S4"
      },
      "outputs": [],
      "source": [
        "# Counting the tokens in the sentence\n",
        "bag_of_words = Counter(tokens)\n",
        "bag_of_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_kHcsSa1S4"
      },
      "outputs": [],
      "source": [
        "bag_of_words.most_common(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njZV7QNSa1S5"
      },
      "source": [
        "Getting the term frequency (**tf**) for a specific word in a document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWo_NxWJa1S5"
      },
      "outputs": [],
      "source": [
        "times_harry_appears = bag_of_words['harry']\n",
        "num_unique_words = len(bag_of_words)         # what is this doing?\n",
        "\n",
        "print(\"Times harry appears:\", times_harry_appears)\n",
        "print(\"Size of the vocabulary:\", num_unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GqVbNvTa1S6"
      },
      "source": [
        "Back to the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbhXuQEZa1S6"
      },
      "source": [
        "### Normalising $tf$: dividing by the total number of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgWjVDsEa1S6"
      },
      "outputs": [],
      "source": [
        "tf = times_harry_appears / num_unique_words\n",
        "tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u540wVx9a1S6"
      },
      "source": [
        "**Note**: I have usually seen:\n",
        "\n",
        "$tf(w') = \\frac{freq(w')}{\\sum_W freq(w)}$\n",
        "\n",
        "(look at the denominator)\n",
        "\n",
        "That is, the sum of all the frequencies.\n",
        "I'm sticking to the book, which uses length of the vocabulary instead. With large vocabularies/collections, it should be roughly equivalent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgcmIWRKa1S7"
      },
      "source": [
        "## $tf$ on a slightly longer text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EEzd9wIa1S7"
      },
      "outputs": [],
      "source": [
        "# Wikipedia article on COVID as in early 2022\n",
        "\n",
        "txt = \"\"\"An ongoing armed conflict between Israel and\n",
        "Palestinian militant groups led by Hamas began on 7 October\n",
        "2023, with a coordinated surprise offensive on Israel. The\n",
        "attack began with a rocket barrage of at least 3,000 rockets\n",
        "launched from the Hamas-controlled Gaza Strip against Israel.\n",
        "In parallel, approximately 2,500 Palestinian militants\n",
        "breached the Gaza–Israel barrier, attacking military bases\n",
        "and massacring civilians in neighboring Israeli communities.\n",
        "At least 1,400 Israelis were killed including 260 people who\n",
        "were massacred at a music festival. Unarmed civilian hostages\n",
        "and captured Israeli soldiers were taken to the Gaza Strip,\n",
        "including women and children. The surprise attack was met with\n",
        "Israeli retaliatory strikes, and Israel formally declared war\n",
        "on Hamas a day later.\n",
        "\n",
        "Israel began clearing Hamas forces from affected areas and\n",
        "conducting airstrikes in the Gaza Strip, which have killed\n",
        "2,339 Palestinians as of 15 October. The United Nations\n",
        "reported that around 1 million Palestinians, nearly half of\n",
        "the population of Gaza, have been internally displaced.\n",
        "Fears of a humanitarian crisis were heightened after Israel\n",
        "cut off food, water, electricity, and fuel supplies to Gaza,\n",
        "which had already been blockaded by both Egypt and Israel.\n",
        "Israel sent messages urging 1.1 million Gazans to evacuate the\n",
        "northern portion of Gaza, while Hamas called on residents to\n",
        "stay put in their homes and blocked roads leading south.\n",
        "\n",
        "There have been widespread deaths of civilians, and a panel of\n",
        "UN independent experts along with a number of human rights\n",
        "groups have accused both Hamas and Israel of committing war\n",
        "crimes during the conflict. At least forty-four countries have\n",
        "denounced Hamas and labeled its conduct as terrorism, while\n",
        "countries across the Middle East called for de-escalation and\n",
        "have attributed the root cause to Israel's decades-long\n",
        "occupation of the Palestinian territories. Iran threatened\n",
        "Israel to immediately stop the war on Gaza. A conflict was\n",
        "reported between militants in Lebanon, including Hezbollah,\n",
        "and Israeli forces on 8 and 9 October. The United States\n",
        "deployed two aircraft carrier battle groups to the Eastern\n",
        "Mediterranean, the United Kingdom declared it would send\n",
        "warships and aircraft, and Germany began supplying military\n",
        "aid to Israel.\n",
        "\"\"\"\n",
        "\n",
        "kk2 = \"\"\"The Russo-Ukrainian War is an ongoing war primarily\n",
        "involving Russia, pro-Russian forces, and Belarus on one side,\n",
        "and Ukraine and its international supporters on the other.\n",
        "Conflict began in February 2014 following the Revolution of\n",
        "Dignity, and focused on the status of Crimea and parts of the\n",
        "Donbas, internationally recognised as part of Ukraine. The\n",
        "conflict includes the Russian annexation of Crimea (2014),\n",
        "the war in Donbas (2014–present), naval incidents, cyberwarfare,\n",
        "and political tensions. Intentionally concealing its involvement,\n",
        "Russia gave military backing to separatists in the Donbas from\n",
        "2014 onwards. Having built up a large military presence on the\n",
        "border from late 2021, Russia launched a full-scale invasion of\n",
        "Ukraine on 24 February 2022, which is ongoing.\n",
        "\n",
        "Following the Euromaidan protests and a revolution resulting in\n",
        "the removal of pro-Russian President Viktor Yanukovych on 22\n",
        "February 2014, pro-Russian unrest erupted in parts of Ukraine.\n",
        "Russian soldiers without insignia took control of strategic\n",
        "positions and infrastructure in the Ukrainian territory of\n",
        "Crimea. Unmarked Russian troops seized the Crimean Parliament\n",
        "and Russia organized a widely-criticised referendum, whose\n",
        "outcome was for Crimea to join Russia. It then annexed Crimea.\n",
        "In April 2014, demonstrations by pro-Russian groups in the\n",
        "Donbas region of Ukraine escalated into a war between the\n",
        "Ukrainian military and Russian-backed separatists of the\n",
        "self-declared Donetsk and Luhansk republics.\n",
        "\n",
        "In August 2014, unmarked Russian military vehicles crossed the\n",
        "border into the Donetsk republic. An undeclared war began between\n",
        "Ukrainian forces and separatists intermingled with Russian troops,\n",
        "although Russia denied the presence of its troops in the Donbas.\n",
        "The war settled into a stalemate, with repeated failed attempts\n",
        "at ceasefire. In 2015, a package of agreements called Minsk II\n",
        "were signed by Russia and Ukraine, but a number of disputes\n",
        "prevented them from being fully implemented. By 2019, 7% of\n",
        "Ukraine's territory was classified by the Ukrainian government\n",
        "as temporarily occupied territories, while the Russian government\n",
        "had indirectly acknowledged the presence of its troops in Ukraine.\n",
        "\n",
        "In 2021 and early 2022, there was a major Russian military\n",
        "build-up around Ukraine's borders. NATO accused Russia of planning\n",
        "an invasion, which it denied. Russian President Vladimir Putin\n",
        "criticized the enlargement of NATO as a threat to his country and\n",
        "demanded Ukraine be barred from ever joining the military\n",
        "alliance. He also expressed Russian irredentist views, questioned\n",
        "Ukraine's right to exist, and stated Ukraine was wrongfully\n",
        "created by Soviet Russia. On 21 February 2022, Russia officially\n",
        "recognised the two self-proclaimed separatist states in the Donbas,\n",
        "and sent troops to the territories. Three days later, Russia\n",
        "invaded Ukraine after Putin announced a \"special military\n",
        "operation\". Much of the international community and organizations\n",
        "such as Amnesty International have condemned Russia for its actions\n",
        "in post-revolutionary Ukraine, accusing it of breaking\n",
        "international law and violating Ukrainian sovereignty. Many\n",
        "countries implemented economic sanctions against Russia, Russian\n",
        "individuals, or companies, especially after the 2022 invasion.\n",
        "\"\"\"\n",
        "\n",
        "kk = \"\"\"Coronavirus disease 2019 (COVID-19) is an infectious disease caused by\n",
        "severe acute respiratory syndrome coronavirus 2 (SARS coronavirus 2,\n",
        "or SARS-CoV-2), a virus closely related to the SARS virus. The disease\n",
        "was discovered and named during the 2019–20 coronavirus outbreak.\n",
        "Those affected may develop a fever, dry cough, fatigue, and shortness\n",
        "of breath. A sore throat, runny nose or sneezing is less common. While\n",
        "the majority of cases result in mild symptoms, some can progress to\n",
        "pneumonia and multi-organ failure.\n",
        "\n",
        "The infection is spread from one person to others via respiratory droplets\n",
        "produced from the airways, often during coughing or sneezing. Time from exposure\n",
        "to onset of symptoms is generally between 2 and 14 days, with an average of 5\n",
        "days. The standard method of diagnosis is by reverse transcription polymerase\n",
        "chain reaction (rRT-PCR) from a nasopharyngeal swab or sputum sample, with\n",
        "results within a few hours to 2 days. Antibody assays can also be used, using a\n",
        "blood serum sample, with results within a few days. The infection can also be\n",
        "diagnosed from a combination of symptoms, risk factors and a chest CT scan\n",
        "showing features of pneumonia.\n",
        "\n",
        "Correct handwashing technique, maintaining distance from people who are coughing\n",
        "and not touching one's face with unwashed hands are measures recommended to\n",
        "prevent the disease. It is also recommended to cover one's nose and mouth with a\n",
        "tissue or a bent elbow when coughing. Masks are also recommended for those who\n",
        "are taking care of someone with a suspected infection but not for the general\n",
        "public. There is no vaccine or specific antiviral treatment, with management\n",
        "involving treatment of symptoms, supportive care and experimental measures.\n",
        "The case fatality rate is estimated at between 1% and 3%.\n",
        "\n",
        "The World Health Organization (WHO) has declared the 2019–20 coronavirus\n",
        "outbreak a Public Health Emergency of International Concern (PHEIC). As of 7\n",
        "March 2020, evidence of local transmission of the disease has been found in\n",
        "multiple countries across all six WHO regions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC3b7kaMa1S7"
      },
      "outputs": [],
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(txt.lower())\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "# display the most common tokens in the document\n",
        "token_counts.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIfZ-gFea1S8"
      },
      "source": [
        "Notice: the most frequent words are (almost) all **stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE5BLNWBa1S8"
      },
      "outputs": [],
      "source": [
        "# Run only the first time\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GISJol2ca1S8"
      },
      "outputs": [],
      "source": [
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "# pay attention to what is going on here (do we need to unfold it?)\n",
        "tokens = [x for x in tokens if x not in stopwords]\n",
        "counts = Counter(tokens)\n",
        "counts.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXmN05Csa1S8"
      },
      "source": [
        "**Homework**:\n",
        "\n",
        "1. Do the same exercise, but using spacy for the pre-processing\n",
        "2. Do the same exercise, but grab an article in Italian\n",
        "3. Play a bit with texts _kk_ and _kk2_\n",
        "3. Normalise these frequencies\n",
        "\n",
        "back to the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF8OhE4za1S8"
      },
      "source": [
        "## Vectorising\n",
        "\n",
        "Not a **dictionary** of counts, but a **vector** of counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8beYATm8a1S8"
      },
      "outputs": [],
      "source": [
        "document_vector = []\n",
        "doc_length = len(tokens)\n",
        "for key, value in counts.most_common():\n",
        "    document_vector.append(value / doc_length)\n",
        "\n",
        "document_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqxH0Wpqa1S9"
      },
      "outputs": [],
      "source": [
        "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
        "docs.append(\"Harry is hairy and faster than Jill.\")\n",
        "docs.append(\"Jill is not as hairy as Harry.\")\n",
        "print(docs, \"\\n\")\n",
        "print(\"\\n\".join(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE1D2AlAa1S9"
      },
      "outputs": [],
      "source": [
        "# Getting the full lexicon\n",
        "doc_tokens = []\n",
        "for doc in docs:\n",
        "    #doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
        "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))\n",
        "\n",
        "print(len(doc_tokens[0]))\n",
        "doc_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBDc-ZGCa1S9"
      },
      "outputs": [],
      "source": [
        "# concatenating all lists into one\n",
        "all_doc_tokens = sum(doc_tokens, [])\n",
        "print(len(all_doc_tokens))\n",
        "all_doc_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab02aHFga1S9"
      },
      "outputs": [],
      "source": [
        "lexicon = sorted(set(all_doc_tokens))\n",
        "print(len(lexicon))\n",
        "\n",
        "# Notice that usually we call the lexicon \"types\" (tokens vs types)\n",
        "lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw7zlXxSa1S9"
      },
      "source": [
        "So, it is clear now: our vectors must have 18 values\n",
        "\n",
        "Creating the initial zero vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRyn2-c4a1S9"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# Creating a template dictionary with all zeros (\"base vector\")\n",
        "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
        "zero_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNaWnz5na1S9"
      },
      "outputs": [],
      "source": [
        "# 1. Make copies of the base vector\n",
        "# 2. Update the values of the vector for each document\n",
        "# 3. Store them in an array\n",
        "#\n",
        "# We use copy.copy to avoid reference copies and do independent copies\n",
        "\n",
        "doc_vectors = []\n",
        "for doc in docs:\n",
        "    vec = copy.copy(zero_vector)\n",
        "    tokens = tokenizer.tokenize(doc.lower())\n",
        "    token_counts = Counter(tokens)\n",
        "    for key, value in token_counts.items():\n",
        "        vec[key] = value / len(lexicon)\n",
        "    doc_vectors.append(vec)\n",
        "\n",
        "doc_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ZbtDo4a1S9"
      },
      "source": [
        "Back to the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rSi9Tya1S-"
      },
      "source": [
        "## Cosine similarity to compare two vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-QJQTmxa1S-"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def cosine_sim(vec1, vec2):\n",
        "    \"\"\"\n",
        "    cosine_sim computes the cosine similarity between two vectors\n",
        "    :param vec1: Dictionary with vector (Counter)\n",
        "    :param vec2: Dictionary with vector (Counter)\n",
        "    :return: cosine(vec1, vec2)\n",
        "    \"\"\"\n",
        "\n",
        "    #Convert our dictionaries into lists for easier matching\n",
        "    vec1 = [val for val in vec1.values()]\n",
        "    vec2 = [val for val in vec2.values()]\n",
        "\n",
        "    dot_prod = 0\n",
        "    for i, v in enumerate(vec1):\n",
        "        dot_prod += v * vec2[i]\n",
        "\n",
        "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
        "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
        "    # this parenthesis is important. Why?\n",
        "    return dot_prod / (mag_1 * mag_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RQDR0cra1S-"
      },
      "outputs": [],
      "source": [
        "print(\"cosine(0, 1)=\", cosine_sim(doc_vectors[0], doc_vectors[1]))\n",
        "print(\"cosine(0, 2)=\", cosine_sim(doc_vectors[0], doc_vectors[2]))\n",
        "print(\"cosine(1, 2)=\", cosine_sim(doc_vectors[1], doc_vectors[2]))\n",
        "print(\"cosine(1, 1)=\", cosine_sim(doc_vectors[1], doc_vectors[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ3d_rxZa1S-"
      },
      "source": [
        "**Homework**: what happens if vec1 and vec2 have different cardinalities? Be defensive and get sure to avoid that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj44cDSda1S-"
      },
      "source": [
        "back to the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOVKjUW9a1S-"
      },
      "source": [
        "## Zipf's Law\n",
        "\n",
        "Let us play with the Brown corpus to see Zipf's law. The [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) is one of the classical corpora in English, with 1M+ words, and a good alternative for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP2UsPSCa1S-"
      },
      "outputs": [],
      "source": [
        "# Run only the first time\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyvWRlRTa1S_"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "# tokens\n",
        "brown.words()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oja7YUca1S_"
      },
      "outputs": [],
      "source": [
        "# Including part of speech\n",
        "brown.tagged_words()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwcQRR1Ba1S_"
      },
      "outputs": [],
      "source": [
        "# Size of the Brown corpus (number of tokens)\n",
        "len(brown.words())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkCV7WBWa1S_"
      },
      "outputs": [],
      "source": [
        "# We define a set of punctuation marks to filter them out\n",
        "puncts = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
        "\n",
        "word_list = (x.lower() for x in brown.words() if x not in puncts)\n",
        "token_counts = Counter(word_list)\n",
        "token_counts.most_common(20)\n",
        "\n",
        "#Does this follow Zipf's distribution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjG5l32a1S_"
      },
      "source": [
        "Back to the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkXCvm5_a1S_"
      },
      "source": [
        "## Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B-58q2za1S_"
      },
      "outputs": [],
      "source": [
        "# Loading two sections of document \"kite\" (from NLP in Action)\n",
        "# from nlpia.data.loaders import kite_text, kite_history\n",
        "\n",
        "kite_text = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing\n",
        "  surfaces that react against the air to create lift and drag. A kite consists of\n",
        "  wings, tethers, and anchors. Kites often have a bridle to guide the face of the\n",
        "  kite at the correct angle so the wind can lift it. A kite's wing also may be so\n",
        "  designed so a bridle is not needed; when kiting a sailplane for launch, the\n",
        "  tether meets the wing at a single point. A kite may have fixed or moving anchors.\n",
        "  Untraditionally in technical kiting, a kite consists of tether-set-coupled wing\n",
        "  sets; even in technical kiting, though, a wing in the system is still often\n",
        "  called the kite.\n",
        "\n",
        "  The lift that sustains the kite in flight is generated when air flows around the\n",
        "  kite's surface, producing low pressure above and high pressure below the wings.\n",
        "  The interaction with the wind also generates horizontal drag along the direction\n",
        "  of the wind. The resultant force vector from the lift and drag force components\n",
        "  is opposed by the tension of one or more of the lines or tethers to which the\n",
        "  kite is attached. The anchor point of the kite line may be static or moving\n",
        "  (e.g., the towing of a kite by a running person, boat, free-falling anchors as\n",
        "  in paragliders and fugitive parakites or vehicle).\n",
        "\n",
        "  The same principles of fluid flow apply in liquids and kites are also used\n",
        "  under water.\n",
        "\n",
        "  A hybrid tethered craft comprising both a lighter-than-air balloon as well as\n",
        "  a kite lifting surface is called a kytoon.\n",
        "\n",
        "  Kites have a long and varied history and many different types are flown\n",
        "  individually and at festivals worldwide. Kites may be flown for recreation,\n",
        "  art or other practical uses. Sport kites can be flown in aerial ballet,\n",
        "  sometimes as part of a competition. Power kites are multi-line steerable kites\n",
        "  designed to generate large forces which can be used to power activities such\n",
        "  as kite surfing, kite landboarding, kite fishing, kite buggying and a new\n",
        "  trend snow kiting. Even Man-lifting kites have been made.\n",
        "\"\"\"\n",
        "\n",
        "kite_history = \"\"\"Kites were invented in China, where materials ideal for kite\n",
        "  building were readily available: silk fabric for sail material; fine,\n",
        "  high-tensile-strength silk for flying line; and resilient bamboo for a strong,\n",
        "  lightweight framework.\n",
        "\n",
        "  The kite has been claimed as the invention of the 5th-century BC Chinese\n",
        "  philosophers Mozi (also Mo Di) and Lu Ban (also Gongshu Ban). By 549 AD paper\n",
        "  kites were certainly being flown, as it was recorded that in that year a paper\n",
        "  kite was used as a message for a rescue mission. Ancient and medieval Chinese\n",
        "  sources describe kites being used for measuring distances, testing the wind,\n",
        "  lifting men, signaling, and communication for military operations. The\n",
        "  earliest known Chinese kites were flat (not bowed) and often rectangular.\n",
        "  Later, tailless kites incorporated a stabilizing bowline. Kites were decorated\n",
        "  with mythological motifs and legendary figures; some were fitted with strings\n",
        "  and whistles to make musical sounds while flying. From China, kites were\n",
        "  introduced to Cambodia, Thailand, India, Japan, Korea and the western world.\n",
        "\n",
        "  After its introduction into India, the kite further evolved into the fighter\n",
        "  kite, known as the patang in India, where thousands are flown every year on\n",
        "  festivals such as Makar Sankranti.\n",
        "\n",
        "  Kites were known throughout Polynesia, as far as New Zealand, with the\n",
        "  assumption being that the knowledge diffused from China along with the people.\n",
        "  Anthropomorphic kites made from cloth and wood were used in religious\n",
        "  ceremonies to send prayers to the gods. Polynesian kite traditions are used by\n",
        "  anthropologists get an idea of early \"primitive\" Asian traditions that are\n",
        "  believed to have at one time existed in Asia.\n",
        "\"\"\"\n",
        "\n",
        "kite_intro = kite_text.lower()\n",
        "intro_tokens = tokenizer.tokenize(kite_intro)\n",
        "\n",
        "kite_history = kite_history.lower()\n",
        "history_tokens = tokenizer.tokenize(kite_history)\n",
        "\n",
        "print(intro_tokens)\n",
        "print(history_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jTRTtXxa1S_"
      },
      "outputs": [],
      "source": [
        "intro_total = len(intro_tokens)\n",
        "intro_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5-yzLwqa1TA"
      },
      "outputs": [],
      "source": [
        "history_total = len(history_tokens)\n",
        "history_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m43iJB9ja1TA"
      },
      "outputs": [],
      "source": [
        "intro_tf = {}\n",
        "history_tf = {}\n",
        "\n",
        "intro_counts = Counter(intro_tokens)\n",
        "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
        "\n",
        "history_counts = Counter(history_tokens)\n",
        "history_tf['kite'] = history_counts['kite'] / history_total\n",
        "\n",
        "'Term Frequency of \"kite\" in intro is: {:.4f}'.format(intro_tf['kite'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7abFNFkza1TA"
      },
      "outputs": [],
      "source": [
        "'Term Frequency of \"kite\" in history is: {:.4f}'.format(history_tf['kite'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCmRkl7Na1TA"
      },
      "source": [
        "$freq(kite, intro) \\sim 2 * freq(kite, history)$\n",
        "\n",
        "Is _kite_intro_ more about kites than _kite_history_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1H2IiNxa1TA"
      },
      "outputs": [],
      "source": [
        "# giving perspective by looking at the frequency of other words\n",
        "intro_tf['and'] = intro_counts['and'] / intro_total\n",
        "history_tf['and'] = history_counts['and'] / history_total\n",
        "print('Term Frequency of \"and\" in intro is: {:.4f}'.format(intro_tf['and']))\n",
        "print('Term Frequency of \"and\" in history is: {:.4f}'.format(history_tf['and']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVvn1aJaa1TA"
      },
      "source": [
        "$freq(and, \\cdot)$ is quite similar to $freq(kite, \\cdot)$\n",
        "\n",
        "So, the document is about **kites** and about **ands**?\n",
        "\n",
        "**Homework:** Write a function to compute the _tf_ and call with both intro_tokens and history_tokens instead of writing the same code twice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ht6G9Uua1TB"
      },
      "source": [
        "(back to the slides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ30anbpa1TB"
      },
      "outputs": [],
      "source": [
        "# Number of documents with *\n",
        "num_docs_containing_and = 0\n",
        "for doc in [intro_tokens, history_tokens]:\n",
        "    if 'and' in doc:\n",
        "        num_docs_containing_and += 1\n",
        "\n",
        "num_docs_containing_kite = 0\n",
        "for doc in [intro_tokens, history_tokens]:\n",
        "    if 'kite' in doc:\n",
        "        num_docs_containing_kite += 1\n",
        "\n",
        "num_docs_containing_china = 0\n",
        "for doc in [intro_tokens, history_tokens]:\n",
        "    if 'china' in doc:\n",
        "        num_docs_containing_china += 1\n",
        "\n",
        "print(\"and:\", num_docs_containing_and)\n",
        "print(\"kite:\", num_docs_containing_kite)\n",
        "print(\"china:\", num_docs_containing_china)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNpHdJB4a1TB"
      },
      "outputs": [],
      "source": [
        "# tf(china)\n",
        "intro_tf['china'] = intro_counts['china'] / intro_total\n",
        "history_tf['china'] = history_counts['china'] / history_total\n",
        "\n",
        "print(intro_tf)\n",
        "print(history_tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEPXKvVpa1TB"
      },
      "outputs": [],
      "source": [
        "# idf for all 3\n",
        "num_docs = 2.0\n",
        "idf = {}\n",
        "idf['and'] = num_docs / num_docs_containing_and\n",
        "idf['kite'] = num_docs / num_docs_containing_kite\n",
        "idf['china'] = num_docs / num_docs_containing_china\n",
        "\n",
        "print(idf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l4Udie1a1TB"
      },
      "outputs": [],
      "source": [
        "# tf-idf for the intro\n",
        "\n",
        "intro_tfidf = {}\n",
        "intro_tfidf['and'] = intro_tf['and'] * idf['and']\n",
        "intro_tfidf['kite'] = intro_tf['kite'] * idf['kite']\n",
        "intro_tfidf['china'] = intro_tf['china'] * idf['china']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhRUnzyUa1TB"
      },
      "outputs": [],
      "source": [
        "# tfidf for the history\n",
        "history_tfidf = {}\n",
        "history_tfidf['and'] = history_tf['and'] * idf['and']\n",
        "history_tfidf['kite'] = history_tf['kite'] * idf['kite']\n",
        "history_tfidf['china'] = history_tf['china'] * idf['china']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCLAF4Ifa1TB"
      },
      "outputs": [],
      "source": [
        "print(intro_tfidf)\n",
        "print(history_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mtYOBLoa1TB"
      },
      "source": [
        "**Homework:** Write a function to compute the tf-idf for all the documents in a (small) collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKVOnxqZa1TB"
      },
      "source": [
        "Back to the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIAR2yyVa1TC"
      },
      "source": [
        "## Searching on \"Harry\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZtBOmfEa1TC"
      },
      "outputs": [],
      "source": [
        "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
        "docs.append(\"Harry is hairy and faster than Jill.\")\n",
        "docs.append(\"Jill is not as hairy as Harry.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec2TK9_Za1TC"
      },
      "outputs": [],
      "source": [
        "# not an extremely efficient implementation (quite verbose as well)\n",
        "document_tfidf_vectors = []\n",
        "for doc in docs:\n",
        "    vec = copy.copy(zero_vector)\n",
        "    tokens = tokenizer.tokenize(doc.lower())\n",
        "    token_counts = Counter(tokens)\n",
        "\n",
        "    for key, value in token_counts.items():\n",
        "        docs_containing_key = 0\n",
        "        for _doc in docs:\n",
        "            if key in _doc:\n",
        "                docs_containing_key += 1\n",
        "        tf = value / len(lexicon)\n",
        "        if docs_containing_key:\n",
        "            idf = len(docs) / docs_containing_key\n",
        "        else:\n",
        "            idf = 0\n",
        "        vec[key] = tf * idf\n",
        "    document_tfidf_vectors.append(vec)\n",
        "\n",
        "document_tfidf_vectors\n",
        "# Notice what happened to Harry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O46wtnoMa1TC"
      },
      "outputs": [],
      "source": [
        "query = \"How long does it take to get to the hairy store?\"\n",
        "query_vec = copy.copy(zero_vector)\n",
        "# query_vec = copy.copy(zero_vector)\n",
        "\n",
        "tokens = tokenizer.tokenize(query.lower())\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "for key, value in token_counts.items():\n",
        "    docs_containing_key = 0\n",
        "    for _doc in docs:\n",
        "      if key in _doc.lower():\n",
        "        docs_containing_key += 1\n",
        "    if docs_containing_key == 0:\n",
        "        continue\n",
        "    tf = value / len(tokens)\n",
        "    idf = len(docs) / docs_containing_key\n",
        "    query_vec[key] = tf * idf\n",
        "query_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJG3GC46a1TC"
      },
      "outputs": [],
      "source": [
        "cosine_sim(query_vec, document_tfidf_vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHQtCDzla1TC"
      },
      "outputs": [],
      "source": [
        "cosine_sim(query_vec, document_tfidf_vectors[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLoeiAdJa1TC"
      },
      "outputs": [],
      "source": [
        "cosine_sim(query_vec, document_tfidf_vectors[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E9X25FXa1TC"
      },
      "outputs": [],
      "source": [
        "print(docs)\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPIPvqB6a1TD"
      },
      "source": [
        "back to the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7To_Pwea1TD"
      },
      "source": [
        "## Using sklearn to build tf-idf matrices\n",
        "\n",
        "Now, rather than implementing everything ourselves, we will use a well-known python library to compute it for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7YO5mwKa1TD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW0LQbEba1TD"
      },
      "outputs": [],
      "source": [
        "# Same small corpus as before\n",
        "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
        "docs.append(\"Harry is hairy and faster than Jill.\")\n",
        "docs.append(\"Jill is not as hairy as Harry.\")\n",
        "corpus = docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECurM76xa1TD"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=1)\n",
        "\"\"\"min_df: ignore terms that have a document frequency\n",
        "strictly lower than the given threshold (aka cut-off).\n",
        "\"\"\"\n",
        "\n",
        "model = vectorizer.fit_transform(corpus)\n",
        "\"\"\"model is a sparse tf-idf matrix (mostly zeros)\n",
        "sklearn does not store zeros to save resources\"\"\"\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW6ldeEla1TD"
      },
      "outputs": [],
      "source": [
        "# We can convert it into a matrix in one line!\n",
        "print(\"\\n--\\n\".join(corpus), \"\\n\")\n",
        "print(model.todense().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NMIzBqqa1TD"
      },
      "source": [
        "## End of the notebook"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}