{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwmAYxOcnVui"
   },
   "source": [
    "# Natural Language Processing 2025.\n",
    "## LM Specialised Translation\n",
    "\n",
    "### Topic Modeling\n",
    "\n",
    "These materials are derived from [Lane et al. (2019)](https://www.manning.com/books/natural-language-processing-in-action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wvhbuy8DnVuj"
   },
   "outputs": [],
   "source": [
    "# This time, I will import the dependencies\n",
    "# the first time they are used (which is not necessarily a good practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1_5rQD3nVuj"
   },
   "source": [
    "## Thought Exercise\n",
    "\n",
    "Training a topic model with _common sense_\n",
    "\n",
    "1. Produce a random tf-idf representation of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k2uaHU1nVuk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic = {}\n",
    "# Function zip() returns an iterator of tuples, where the i-th tuple\n",
    "# contains the i-th element from each of the argument sequences\n",
    "\n",
    "# np.random.rand(i) produces an array of i random numbers\n",
    "\n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(),\n",
    "                      np.random.rand(6)))\n",
    "            )\n",
    "# Random tf-idf vector for our single document\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56snkF2inVuk"
   },
   "source": [
    "2. __Manually__ create common-sense weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_qc9o4GnVuk"
   },
   "outputs": [],
   "source": [
    "# Now, we multiply the tf-idf vector by the\n",
    "# \"hand-crafted” weights (notice the subtractions)\n",
    "topic['petness'] = (\n",
    "                  .3 * tfidf['cat']\n",
    "                + .3 * tfidf['dog']\n",
    "                +  0 * tfidf['apple']\n",
    "                +  0 * tfidf['lion']\n",
    "                - .2 * tfidf['NYC']\n",
    "                + .2 * tfidf['love'])\n",
    "topic['animalness'] = (\n",
    "                  .1 * tfidf['cat']\n",
    "                + .1 * tfidf['dog']\n",
    "                - .1 * tfidf['apple']\n",
    "                + .5 * tfidf['lion']\n",
    "                + .1 * tfidf['NYC']\n",
    "                - .1 * tfidf['love'])\n",
    "topic['cityness'] = (\n",
    "                   0 * tfidf['cat']\n",
    "                - .1 * tfidf['dog']\n",
    "                + .2 * tfidf['apple']\n",
    "                - .1 * tfidf['lion']\n",
    "                + .5 * tfidf['NYC']\n",
    "                + .1 * tfidf['love'])\n",
    "topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O27STz1cnVuk"
   },
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtnXWSm8nVuk"
   },
   "source": [
    "3. Transposing the 6x3 matrix to produce **topic weights for each word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3bIZqQynVuk"
   },
   "outputs": [],
   "source": [
    "word_vector = {}\n",
    "\n",
    "word_vector['cat'] = [.3*topic['petness'],\n",
    "                    .1*topic['animalness'],\n",
    "                    0*topic['cityness']]\n",
    "\n",
    "word_vector['dog'] = [.3*topic['petness'],\n",
    "                    .1*topic['animalness'],\n",
    "                    -.1*topic['cityness']]\n",
    "\n",
    "word_vector['apple']= [0*topic['petness'],\n",
    "                    .1*topic['animalness'],\n",
    "                    .2*topic['cityness']]\n",
    "\n",
    "word_vector['lion'] = [0*topic['petness'],\n",
    "                    .5*topic['animalness'],\n",
    "                    -.1*topic['cityness']]\n",
    "word_vector['NYC'] = [-.2*topic['petness'],\n",
    "                    .1*topic['animalness'],\n",
    "                    .5*topic['cityness']]\n",
    "word_vector['love'] = [.2*topic['petness'],\n",
    "                    -.1*topic['animalness'],\n",
    "                    .1*topic['cityness']]\n",
    "word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHRLeOPNnVul"
   },
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBzdtyLFnVul"
   },
   "source": [
    "## Training a Linear Discriminant Analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uR-pruGtnVul"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading a labeled corpus: spam (this was with the dependency of nlpia)\n",
    "# from nlpia.data.loaders import get_data\n",
    "# sms = get_data('sms-spam')\n",
    "\n",
    "# We can download it directly from their repo\n",
    "url = \"https://raw.githubusercontent.com/totalgood/nlpia/master/src/nlpia/data/sms-spam.csv\"\n",
    "sms = pd.read_csv(url)\n",
    "print(sms[:-10])\n",
    "# Just setting up the printing properties\n",
    "pd.options.display.width = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SriozPZ4nVul"
   },
   "outputs": [],
   "source": [
    "# For display purposes: spam instances have a \"!\" added to the label\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in \\\n",
    "         zip(range(len(sms)), sms.spam)]\n",
    "print(index[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDboXSzSnVul"
   },
   "outputs": [],
   "source": [
    "#'!'*0\n",
    "#'!'*1\n",
    "#'!'*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHmum364nVul"
   },
   "outputs": [],
   "source": [
    "# Creating a pandas df, using the data and the new index\n",
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms['spam'] = sms.spam.astype(int)\n",
    "print(sms)\n",
    "# len(sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3vQRBvdnVul"
   },
   "outputs": [],
   "source": [
    "# QUESTION: what am I getting with this sum?\n",
    "sms.spam.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIyv1yXMnVul"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorising the corpus\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=sms.text).toarray()\n",
    "# QUESTION: what is the number on the right?\n",
    "print(tfidf_docs.shape)\n",
    "print(tfidf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8sivdnbnVul"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "We have\n",
    "* 4837 messages\n",
    "* 638 positive instances\n",
    "* 9232 types\n",
    "\n",
    "That's way too much for a Naïve Bayes' classifier\n",
    "\n",
    "**Homework**: try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhbwQ8N6nVum"
   },
   "source": [
    "## Implementing Linear Discriminant Analysis \n",
    "\n",
    "We just need the centroids of spam and non-spam, so we implement it\n",
    "\n",
    "(keep in mind that sklearn has an [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8yZ4FfunVum"
   },
   "outputs": [],
   "source": [
    "# A mask (or \"filter\") to select only spam messages\n",
    "mask = sms.spam.astype(bool).values\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O99nDQaFnVum"
   },
   "outputs": [],
   "source": [
    "# Computing the spam centroid\n",
    "spam_centroid = tfidf_docs[mask].mean(axis=0)\n",
    "# axis=0 tells numpy to compute the mean for each column independently\n",
    "print(spam_centroid.round(2))\n",
    "len(spam_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIuLYk8wnVum"
   },
   "outputs": [],
   "source": [
    "# Computing the ham centroid\n",
    "ham_centroid = tfidf_docs[~mask].mean(axis=0)\n",
    "print(ham_centroid.round(2))\n",
    "len(ham_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VO-KWx3jnVum"
   },
   "outputs": [],
   "source": [
    "spam_centroid - ham_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1dJ_msMnVum"
   },
   "outputs": [],
   "source": [
    "# Computing the centroid difference: \"the line between spam and ham\"\n",
    "spamminess_score = tfidf_docs.dot(spam_centroid - ham_centroid)\n",
    "print(spamminess_score.round(2))\n",
    "len(spamminess_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiuH5KBXnVum"
   },
   "source": [
    "Not just subtracting. We computed the dot product!\n",
    "\n",
    "**spamminess_score** is $dis(centroid_{(spam)}, centroid_{(ham)})$\n",
    "\n",
    "We compute it by projecting each TF-IDF vector onto that line between the centroids using the dot product (those were indeed 4,837 dot products computed at once!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUigA417nVum"
   },
   "outputs": [],
   "source": [
    "# Convert the vector spamminess_score into matrix:\n",
    "spamminess_score.reshape(-1,1)\n",
    "# Because the input to the next function must have shape: (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnOPLD9unVum"
   },
   "source": [
    "[MinMaxScaler's fit_transform](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.fit_transform) will scale each of the features to range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQX9544HnVum"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Turning the scores into \"probabilities\"\n",
    "sms['lda_score'] = MinMaxScaler().fit_transform(spamminess_score.reshape(-1,1))\n",
    "\n",
    "# Turning them into predictions\n",
    "sms['lda_predict'] = (sms.lda_score > .5).astype(int)\n",
    "\n",
    "# Displaying them\n",
    "sms['spam lda_predict lda_score'.split()].round(2).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHOzMHUenVum"
   },
   "outputs": [],
   "source": [
    "# What is the accuracy of the model?\n",
    "(1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEtKBWDknVum"
   },
   "outputs": [],
   "source": [
    "# Getting a confusion matrix (the nlpai way)\n",
    "# from pugnlp.stats import Confusion\n",
    "# Confusion(sms['spam lda_predict'.split()])\n",
    "\n",
    "# we do it with sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(sms.spam, sms.lda_predict)\n",
    "\n",
    "#          pred_spam   pred_no-spam\n",
    "# spam       a              b\n",
    "# no-spam    c              d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYsGSYzQnVun"
   },
   "source": [
    "**end of the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOfBC6dsnVun"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
