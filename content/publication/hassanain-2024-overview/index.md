---
title: Overview of the CLEF-2024 CheckThat! Lab Task 1 on Check-Worthiness Estimation
  of Multigenre Content
authors:
- Maram Hasanain
- Reem Suwaileh
- Sanne Weering
- Chengkai Li
- Tommaso Caselli
- Wajdi Zaghouani
- Alberto Barrón-Cedeño
- Preslav Nakov
- Firoj Alam
date: '2024-01-01'
publishDate: '2025-03-28T21:37:45.387949Z'
publication_types:
- paper-conference
publication: '*Working Notes of the Conference and Labs of the Evaluation Forum (CLEF
  2024)*'
abstract: 'We present an overview of the CheckThat! Lab 2024 Task 1, part of CLEF
  2024. Task 1 involves determining whether a text item is check-worthy, with a special
  emphasis on COVID-19, political news, and political debates and speeches. It is
  conducted in three languages: Arabic, Dutch, and English. Additionally, Spanish
  was offered for extra training data during the development phase. A total of 75
  teams registered, with 37 teams submitting 236 runs and 17 teams submitting system
  description papers. Out of these, 13, 15 and 26 teams participated for Arabic, Dutch
  and English, respectively. Among these teams, the use of transformer pre-trained
  language models (PLMs) was the most frequent. A few teams also employed Large Language
  Models (LLMs). We provide a description of the dataset, the task setup, including
  evaluation settings, and a brief overview of the participating systems. As is customary
  in the CheckThat! Lab, we release all the datasets as well as the evaluation scripts
  to the research community. This will enable further research on identifying relevant
  check-worthy content that can assist various stakeholders, such as fact-checkers,
  journalists, and policymakers.'
tags:
- Check-worthiness
- fact-checking
- multilinguality
---
