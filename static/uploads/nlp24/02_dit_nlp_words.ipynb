{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrg_RmuhyO1E"
      },
      "source": [
        "# Natural Language Processing 2024.\n",
        "## LM Specialised Translation\n",
        "\n",
        "\n",
        "### A quick overview on preprocessing\n",
        "\n",
        "These materials are mostly borrowed from [Lane et al. (2019)](https://www.manning.com/books/natural-language-processing-in-action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np8oMy8VyO1G"
      },
      "source": [
        "We first need to import dependencies. In this case, there regex module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQM4tRFryO1H"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QqtkntuyO1H"
      },
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7jfR9p8yO1I"
      },
      "outputs": [],
      "source": [
        "txt = \"Thomas Jefferson started building Monticello at the age of 26.\"\n",
        "# What is the difference between \" \" and \"\"\" \"\"\" ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZPnp2ntyO1I"
      },
      "source": [
        "A simple \"tokeniser\", which captures alphabetical characters only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9pUK7x3yO1I"
      },
      "outputs": [],
      "source": [
        "tokens = re.findall('[A-Za-z]+', txt)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMn-8qleyO1J"
      },
      "source": [
        "Python provides a \"similar\" tool to tokenise. It is strings function `split()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSIruTN3yO1J"
      },
      "outputs": [],
      "source": [
        "tokens = txt.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQS345q3yO1K"
      },
      "source": [
        "Still, in general, it is not enough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqx1YsWcyO1K"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_tbcE3hyO1K"
      },
      "source": [
        " Obviously, we can design a better regular expression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyvXdBLhyO1K"
      },
      "outputs": [],
      "source": [
        "tokens = re.split(r'([-\\s.,;!?])+', txt)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiYHkAiOyO1L"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp2acY1kyO1L"
      },
      "outputs": [],
      "source": [
        "text = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987\"\n",
        "tokens = re.split(r'([-\\s.,;!?])+', text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9md_64LjyO1L"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6cWmzm6yO1L"
      },
      "source": [
        "### Libraries\n",
        "\n",
        "The community has created multiple libraries for pre-processing, which include fucntions to perform tokenisation and many other operations.\n",
        "\n",
        "Two of the most popular ones are\n",
        "\n",
        "* [NLTK](http://www.nltk.org)\n",
        "* [Spacy](https://spacy.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhZQBdQ9yO1L"
      },
      "source": [
        "If it is the first time you use them (and this is mostly true if you are using an ephimerous platform, such as colab), you should install it.\n",
        "\n",
        "You can do so with [pip](https://pip.pypa.io/en/stable/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjEjg4PayO1L"
      },
      "outputs": [],
      "source": [
        "!pip install --user -U spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY0seAAOyO1L"
      },
      "source": [
        "If you are working from the terminal, in local, you might have to do like this (for all dependencies)\n",
        "\n",
        "```\n",
        "$ pip install --user -U spacy\n",
        "```\n",
        "\n",
        "--user tels pip to install it only for you; -U tells to upgrade the package (if it was already installed)\n",
        "\n",
        "**Note:** typically you install all the dependencies on top of the notebook, as it is the first thing you should do\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ijXtiw2yO1M"
      },
      "source": [
        "An now we can import and use one of its tokenisers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QlmlSi_myO1M"
      },
      "outputs": [],
      "source": [
        "# loading the library\n",
        "import spacy\n",
        "\n",
        "# downloading the model\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YTYOOJHyO1M"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(txt)\n",
        "print([token.text for token in doc])\n",
        "\n",
        "# Here is the equivalent process, using NLTK\n",
        "# from nltk.tokenize import TreebankWordTokenizer # import one of the many tokenizers available\n",
        "# tokenizer = TreebankWordTokenizer()             # invoke it\n",
        "# tokens = tokenizer.tokenize(txt)\n",
        "# print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeC5bSBSyO1M"
      },
      "source": [
        "Now, see the difference between tokenising with split() and with spacy's web tokeniser on a different sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-JP5pQYyO1M"
      },
      "outputs": [],
      "source": [
        "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
        "tokens_split = sentence.split()\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(\"OUTPUT USING split()\\t\", tokens_split)\n",
        "print(\"OUTPUT USING spacy\\t\", [token.text for token in  doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfAHmaBwyO1N"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjXvyPOQyO1N"
      },
      "source": [
        "## Normalisation\n",
        "\n",
        "### Casefolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOOEvQMkyO1N"
      },
      "outputs": [],
      "source": [
        "sentence  = sentence.lower()\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXUJzojZyO1N"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT8q8_ZsyO1N"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "Once again, we can use a regular expression to do stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy4108fDyO1N"
      },
      "outputs": [],
      "source": [
        "def stem(phrase):\n",
        "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
        "         word)[0][0].strip(\"'\") for word in phrase.lower()\n",
        "         .split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suNaFug1yO1N"
      },
      "outputs": [],
      "source": [
        "print(\"'houses' \\t\\t->\", stem('houses'))\n",
        "print(\"'Doctor House's admin staff calls' \\t->\", stem(\"Doctor House's admin staff calls\"))\n",
        "print(\"'stress' \\t\\t->\", stem(\"stress\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0RdHEvWyO1O"
      },
      "source": [
        "But we would need to include many more expressions to deal with all cases and exceptions.\n",
        "\n",
        "Instead, once again we can rely on a library. Let's consider the **Porter stemmer**. NLTK has an implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfSQf8keyO1O"
      },
      "outputs": [],
      "source": [
        "# Installing NLTK (and its dependency: numpy)\n",
        "! pip install --user -U nltk\n",
        "! pip install --user -U numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-rRqXaRyO1O"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer # Import the stemmer\n",
        "stemmer = PorterStemmer()                  # invoke the stemmer\n",
        "\n",
        "# Notice:\n",
        "# - this one-liner \"tokenises\", stems, and concatenates, all in one line!\n",
        "# - these operations \"appear\" inverted in the code (let us have a look together)\n",
        "x = ' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])\n",
        "print(x.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaqFUn5-yO1O"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuO-YXRYyO1O"
      },
      "source": [
        "## Lemmatisation\n",
        "\n",
        "This is a more complex process, compared to stemming. Let us use a library.\n",
        "In this particular case we are going to use NLTK's WordNet lemmatiser. If it is the first time you use it (or you are in an ephemeral environment!), you should download it as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrsUZa5DyO1O"
      },
      "source": [
        "### The NLTK alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNQYPZPEyO1O"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "## Download the Wordnet resources\n",
        "# WordNet core resources\n",
        "nltk.download('wordnet')\n",
        "# Open Multilingual Wordnet resources\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svDKaaJ6yO1P"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer # importing the lemmatiser\n",
        "lemmatizer = WordNetLemmatizer()        # invoking the lemmatiser\n",
        "\n",
        "print(\"'better' alone \\t->\",lemmatizer.lemmatize(\"better\"))\n",
        "print(\"'better' incl. it's POS (adj) \\t->\",lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzNUQDRuyO1P"
      },
      "source": [
        "### The Spacy alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW_bK1m_yO1P"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"better\")\n",
        "print([token.lemma_ for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipOfaqymyO1P"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsenXctQyO1P"
      },
      "source": [
        "## A quick overview on representations\n",
        "\n",
        "### Bag of Words (BoW)\n",
        "\n",
        "First, let us see a simple construction, using a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSoRZmUayO1P"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26. Thomas\"\"\"\n",
        "\n",
        "sentence_bow = {}\n",
        "for token in sentence.split():\n",
        "     sentence_bow[token] = 1\n",
        "sorted(sentence_bow.items())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiVkBXFKyO1P"
      },
      "source": [
        "**Back to the slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCWRrrXXyO1P"
      },
      "source": [
        "Another option would be using **pandas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyoQLV5nyO1Q"
      },
      "outputs": [],
      "source": [
        "# You might have to install it first\n",
        "! pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDMzQwCVyO1Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading the corpus\n",
        "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
        "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
        "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
        "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
        "\n",
        "# Loading the tokens into a dictionary (notice that we asume that each line is a document)\n",
        "corpus = {}\n",
        "for i, sent in enumerate(sentences.split('\\n')):\n",
        "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in\n",
        "         sent.split())\n",
        "\n",
        "# Loading the dictionary contents into a pandas dataframe.\n",
        "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
        "# SEE THE .T, which transposes the matrix for visualisation purposes.\n",
        "\n",
        "\n",
        "df[df.columns[:10]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDn-eG7qyO1Q"
      },
      "source": [
        "### One-hot vectors\n",
        "\n",
        "This is our input sentence (and its vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xYpXOhryO1Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
        "token_sequence = str.split(sentence)\n",
        "vocab = sorted(set(token_sequence))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IKbqM0-yO1Q"
      },
      "source": [
        "And now, we produce the one-hot representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzIqWQ6IyO1Q"
      },
      "outputs": [],
      "source": [
        "num_tokens = len(token_sequence)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# create the |tokens| x |vocabulary size| matrix of zeros\n",
        "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
        "print(token_sequence)\n",
        "print(onehot_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNXTEnkHyO1Q"
      },
      "outputs": [],
      "source": [
        "for i, word in enumerate(token_sequence):\n",
        "   onehot_vectors[i, vocab.index(word)] = 1  # switch on (1) the right element of the vector\n",
        "\n",
        "print(\"Vocabulary:\\t\", vocab)\n",
        "print(\"Sentence:\\t\", token_sequence)\n",
        "onehot_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2aChnzCyO1Q"
      },
      "source": [
        "Let us bring **pandas** into the game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eusLUTWUyO1Q"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(onehot_vectors, columns=vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUfqAnm0yO1Q"
      },
      "source": [
        "## Defining the preprocessing _pipeline_:\n",
        "\n",
        "1. Tokenisation\n",
        "2. Stemmming\n",
        "3. Stopwording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3RPBWq_yO1R"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1xNDeN-yO1R"
      },
      "outputs": [],
      "source": [
        "# invoking the necessary objects\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxWosFUryO1R"
      },
      "outputs": [],
      "source": [
        "# a tiny test\n",
        "print(tokenizer.tokenize(\"The input text.\"))\n",
        "stemmer.stem(\"documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wLX30o7yO1R"
      },
      "outputs": [],
      "source": [
        "# both tokenisation and stemming\n",
        "text = \"\"\"Perseverance (nicknamed Percy) is a car-sized Mars\n",
        "rover designed to explore the crater Jezero on Mars as part\n",
        "of NASA's Mars 2020 mission.\"\"\"\n",
        "\n",
        "print([stemmer.stem(w) for w in tokenizer.tokenize(text)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYr2FgzTyO1R"
      },
      "source": [
        "### Stopwording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASZ54P8RyO1R"
      },
      "outputs": [],
      "source": [
        "# The first time you use the stopwords, you have to download them!\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAbsCipsyO1R"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "print(stop_words[:100])\n",
        "stop_words = set(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcISu0v2yO1R"
      },
      "source": [
        "### What is a stopword?\n",
        "\n",
        "According to [the Wikipedia](https://en.wikipedia.org/wiki/Stop_word): \"the words in a stop list (or stoplist or negative dictionary) which are **filtered out** (i.e. stopped) before or after processing of natural language data (text) because they are insignificant.\"\n",
        "\n",
        "For some search engines, these are **some of the most common, short function words,** such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as \"The Who\", \"The The\", or \"Take That\".\n",
        "\n",
        "**Q: Can I create a list of stopwords on the fly?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bKpxjNUyO1S"
      },
      "outputs": [],
      "source": [
        "# tokenisation and stemming, and stopwording\n",
        "text = \"\"\"Perseverance (nicknamed Percy) is a car-sized Mars\n",
        "rover designed to explore the crater Jezero on Mars as part\n",
        "of NASA's Mars 2020 mission.\"\"\"\n",
        "\n",
        "print([stemmer.stem(w) for w in tokenizer.tokenize(text) if w not in stop_words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awBPNmofyO1S"
      },
      "source": [
        "## Homework\n",
        "\n",
        "When each of the components is first introduced, the simplest versions of the others are used. Create a pipeline that performs the whole pre-precessing...\n",
        "\n",
        "1. Using NLTK alone\n",
        "2. Using spacy alone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQTGKmGbyO1S"
      },
      "source": [
        "**End of the notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}