{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CodgW4tN_iV0"
      },
      "source": [
        "# DIT NLP lesson 2024\n",
        "\n",
        "## Using character-level representations for classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NyOEJmW_iV1"
      },
      "outputs": [],
      "source": [
        "# Importing the dependencies\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,  Flatten, LSTM\n",
        "\n",
        "from random import shuffle\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oBjEjoc_iV2"
      },
      "outputs": [],
      "source": [
        "# Add the path to the corpus. It should end in aclImdb/train\n",
        "# CORPUS_PATH = \"/Users/albarron/corpora/misc/stanford_movie_review/aclImdb/train\"\n",
        "PATH_TO_CORPUS = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "CORPUS_FILE_NAME = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "CORPUS_PATH = \"aclImdb/train\"\n",
        "\n",
        "def download_file(url_to_file, path_to_file):\n",
        "  if os.path.isfile(path_to_file):\n",
        "    print(\"A local copy of the file exists already:\", path_to_file, \"\\nDoing nothing\")\n",
        "  else:\n",
        "    request.urlretrieve(url_to_file, path_to_file)\n",
        "\n",
        "# Downloading and untaring the corpus\n",
        "\n",
        "download_file(PATH_TO_CORPUS, CORPUS_FILE_NAME)\n",
        "with tarfile.open(CORPUS_FILE_NAME) as f:\n",
        "  f.extractall(path=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BRESJI0_iV2"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    Load pos and neg examples from separate dirs then shuffle them\n",
        "    together.\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "    shuffle(dataset)\n",
        "    return dataset\n",
        "\n",
        "def collect_expected(dataset):\n",
        "    \"\"\"Extracting the expected output for all the instances\"\"\"\n",
        "    return [sample[0] for sample in dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl2SFpg7_iV2"
      },
      "outputs": [],
      "source": [
        "# Loading instances and expected classes (as usual)\n",
        "dataset = pre_process_data(CORPUS_PATH)\n",
        "expected = collect_expected(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bQv3ZjG_iV2"
      },
      "outputs": [],
      "source": [
        "def avg_len(data):\n",
        "    \"\"\"Computes the average length of the data\"\"\"\n",
        "    total_len = 0\n",
        "    for sample in data:\n",
        "        total_len += len(sample[1])\n",
        "    return total_len/len(data)\n",
        "avg_len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT9twxO6_iV2"
      },
      "source": [
        "The average word length is 202.44 (**Homework:** don't believe me and go find yourself).\n",
        "\n",
        "That is, we would unroll the network **6.5x**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_1rCzZT_iV2"
      },
      "outputs": [],
      "source": [
        "def clean_data(data):\n",
        "    \"\"\"Shift to lower case, replace unknowns with UNK, and listify \"\"\"\n",
        "    new_data = []\n",
        "    VALID = 'abcdefghijklmnopqrstuvwxyz0123456789\"\\'?!.,:; '\n",
        "    for sample in data:\n",
        "        new_sample = []\n",
        "        for char in sample[1].lower():  # Just grab the string, not the label\n",
        "            # Not extremely efficient procedure\n",
        "            if char in VALID:\n",
        "                new_sample.append(char)\n",
        "            else:\n",
        "                new_sample.append('UNK')\n",
        "\n",
        "        new_data.append(new_sample)\n",
        "    return new_data\n",
        "\n",
        "# listified_data = clean_data(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16ND9j3_iV2"
      },
      "source": [
        "**Homework**: turn the process to determine if a character is VALID more efficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jujxxFkb_iV2"
      },
      "outputs": [],
      "source": [
        "def char_pad_trunc(data, maxlen):\n",
        "    \"\"\" We truncate to maxlen or add PAD tokens \"\"\"\n",
        "    new_dataset = []\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            new_data = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            pads = maxlen - len(sample)\n",
        "            new_data = sample + ['PAD'] * pads\n",
        "        else:\n",
        "            new_data = sample\n",
        "        new_dataset.append(new_data)\n",
        "    return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrIZEDzD_iV3"
      },
      "outputs": [],
      "source": [
        "# Producing the one-hot encodings (no embeddings here)\n",
        "def create_dicts(data):\n",
        "    \"\"\" Modified from Keras LSTM example\"\"\"\n",
        "    chars = set()\n",
        "    for sample in data:\n",
        "        chars.update(set(sample))\n",
        "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "    return char_indices, indices_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_Zc0FEB_iV3"
      },
      "outputs": [],
      "source": [
        "def onehot_encode(dataset, char_indices, maxlen):\n",
        "    \"\"\"\n",
        "    One hot encode the tokens\n",
        "\n",
        "    Args:\n",
        "        dataset  list of lists of tokens\n",
        "        char_indices  dictionary of {key=character, value=index to use encoding vector}\n",
        "        maxlen  int  length of each sample\n",
        "    Return:\n",
        "        np array of shape (samples, tokens, encoding length)\n",
        "    \"\"\"\n",
        "    X = np.zeros((len(dataset), maxlen, len(char_indices.keys())))\n",
        "    for i, sentence in enumerate(dataset):\n",
        "        for t, char in enumerate(sentence):\n",
        "            X[i, t, char_indices[char]] = 1\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLxho_XY_iV3"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the data\n",
        "# The first 2 steps were run earlier\n",
        "# dataset = pre_process_data(CORPUS_PATH)\n",
        "# expected = collect_expected(dataset)\n",
        "listified_data = clean_data(dataset)\n",
        "\n",
        "maxlen = 1500\n",
        "common_length_data = char_pad_trunc(listified_data, maxlen)\n",
        "\n",
        "char_indices, indices_char = create_dicts(common_length_data)\n",
        "encoded_data = onehot_encode(common_length_data, char_indices, maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4oHJhHF_iV3"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "split_point = int(len(encoded_data)*.8)\n",
        "\n",
        "x_train = encoded_data[:split_point]\n",
        "y_train = np.array(expected[:split_point])\n",
        "x_test = encoded_data[split_point:]\n",
        "y_test = np.array(expected[split_point:])\n",
        "# Pay attention: in the book they forgot to turn y_[train|test] into numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRryjhOs_iV3"
      },
      "outputs": [],
      "source": [
        "# A quick view to the first instance\n",
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCLGi2pu_iV3"
      },
      "outputs": [],
      "source": [
        "# Shape of the resulting array\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3exvHjzd_iV3"
      },
      "outputs": [],
      "source": [
        "# How many instances do we have?\n",
        "len(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B09OluV_iV3"
      },
      "source": [
        "**Q: What is  the size of the vocabulary?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oaas6TXz_iV3"
      },
      "outputs": [],
      "source": [
        "# Building the network\n",
        "\n",
        "# Embedding,\n",
        "num_neurons = 40\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(\n",
        "    num_neurons,\n",
        "    return_sequences=True,\n",
        "    input_shape=(maxlen, len(char_indices.keys())))\n",
        "    )\n",
        "\n",
        "model.add(Dropout(.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYh-8OhW_iV4"
      },
      "source": [
        "[rmsprop](https://keras.io/api/optimizers/rmsprop/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EotClY2l_iV4"
      },
      "outputs": [],
      "source": [
        "# Training the network\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "model.fit(x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test)\n",
        "    )\n",
        "# This would take between 5 and 10 minutes per epoch, depending on the hardware!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1LD_1Is_iV4"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "model_structure = model.to_json()\n",
        "with open(\"char_lstm_model3.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)\n",
        "model.save_weights(\"char_lstm.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqZaq9I5_iV4"
      },
      "source": [
        "Back to the slides"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}