{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw5jACJmuLSQ"
      },
      "source": [
        "# DIT NLP lesson 2024\n",
        "\n",
        "## CNNs on text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaEklyOTuLSU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os.path\n",
        "import tarfile\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "from keras.preprocessing import sequence   # necessary for padding\n",
        "from keras.models import Sequential        # Base Keras NN model\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D # Convolution layer and pooling\n",
        "from keras.layers import Dense, Dropout, Activation # The objects for each layer\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from urllib import request\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bejGp6_8uLST"
      },
      "source": [
        "For this exercise, we first download Stanford's [Large Movie Review Dataset](https://ai.stanford.edu/%7eamaas/data/sentiment/aclImdb_v1.tar.gz). More information on the corpus at [Learning Word Vectors for Sentiment Analysis](https://ai.stanford.edu/%7eamaas/papers/wvSent_acl2011.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tp25FdWuLSU"
      },
      "outputs": [],
      "source": [
        "PATH_TO_CORPUS = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "CORPUS_FILE_NAME = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "PATH_TO_GOOGLENEWS_VECTORS =\"https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\"\n",
        "GOOGLE_VECTORS = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "CORPUS_PATH = \"aclImdb/train\"\n",
        "\n",
        "def download_file(url_to_file, path_to_file):\n",
        "  if os.path.isfile(path_to_file):\n",
        "    print(\"A local copy of the file exists already:\", path_to_file, \"\\nDoing nothing\")\n",
        "  else:\n",
        "    request.urlretrieve(url_to_file, path_to_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the embeddings\n",
        "\n",
        "download_file(PATH_TO_GOOGLENEWS_VECTORS, GOOGLE_VECTORS)"
      ],
      "metadata": {
        "id": "gmtdGr1mwt5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and untaring the corpus\n",
        "\n",
        "download_file(PATH_TO_CORPUS, CORPUS_FILE_NAME)\n",
        "with tarfile.open(CORPUS_FILE_NAME) as f:\n",
        "  f.extractall(path=\".\")"
      ],
      "metadata": {
        "id": "dPsvTLPNwqso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS9GTF46uLSV"
      },
      "outputs": [],
      "source": [
        "# A method to read and shuffle all instances (one per file).\n",
        "# Positive (negative) instances are in the pos (neg) folder\n",
        "\n",
        "# glob.glob returns a list of path names that match pathname\n",
        "import glob\n",
        "import os\n",
        "from random import shuffle\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    This is dependent on your training data source but we will\n",
        "    try to generalize it as best as possible.\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "    shuffle(dataset)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j933LPb3uLSV"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the data\n",
        "dataset = pre_process_data(CORPUS_PATH)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHPh0YJ_uLSW"
      },
      "outputs": [],
      "source": [
        "# Loading the word2vec embeddings\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format(GOOGLE_VECTORS,\n",
        "    binary=True, limit=400000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCftM2k0uLSX"
      },
      "outputs": [],
      "source": [
        "# Method to tokenise and vectorise all the training data\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "#    expected = [] this line appears in the book, but it's not necessary here!\n",
        "    for sample in dataset:\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(word_vectors[token])\n",
        "            except KeyError:\n",
        "                pass # No matching token in the Google w2v vocab\n",
        "        vectorized_data.append(sample_vecs)\n",
        "\n",
        "    return vectorized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmNj4h6RuLSX"
      },
      "outputs": [],
      "source": [
        "# Method to get the target labels\n",
        "def collect_expected(dataset):\n",
        "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7ZdwNotuLSY"
      },
      "outputs": [],
      "source": [
        "# Vectorising the dataset and extracting the gold standard\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEamyzAcuLSY"
      },
      "outputs": [],
      "source": [
        "# Creating training and validation partitions\n",
        "\n",
        "split_point = int(len(vectorized_data)*.8)\n",
        "x_train = vectorized_data[:split_point] # there's a typo in this line, if copying from the book\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A little trick to release the memory used by vectorized_data\n",
        "vectorized_data = 0\n"
      ],
      "metadata": {
        "id": "q1Z0B2h-0DcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d37C-neQuLSY"
      },
      "outputs": [],
      "source": [
        "# Network parameters\n",
        "\n",
        "maxlen = 400          # maximum length of the text (why?)\n",
        "batch_size = 32       # number of samples before backpropagating\n",
        "embedding_dims = 300  # Same as Google's\n",
        "filters = 250         # (!)\n",
        "kernel_size = 3       # remember: filter=kernel (we have a scalar this time)\n",
        "hidden_dims = 250     # number of neurons in the final layer\n",
        "epochs = 2            # number of training epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCAfR68luLSa"
      },
      "source": [
        "**Back to the slides** to see what is \"padding\" in text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2jG5_z2uLSa"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z48MMvLcuLSa"
      },
      "outputs": [],
      "source": [
        "# Method to pad or truncate the input\n",
        "# (notice that this code is quite verbose)\n",
        "def pad_trunc(data, maxlen):\n",
        "    \"\"\"\n",
        "    For a given dataset pad with zero vectors or truncate to maxlen\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "    # Create a vector of 0s the length of our word vectors\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            temp = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            temp = sample\n",
        "            # Append the appropriate number 0 vectors to the list\n",
        "            additional_elems = maxlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtfKT5fcuLSa"
      },
      "outputs": [],
      "source": [
        "# Padding/truncating the data (if necessary)\n",
        "\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)\n",
        "\n",
        "# The shape is [number of samples, sequence length, word vector]\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUdU26LIuLSa"
      },
      "source": [
        "## Building the network\n",
        "\n",
        "Adding the convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FwkDLbEuLSa"
      },
      "outputs": [],
      "source": [
        "print('Building model...')\n",
        "model = Sequential()   # The standard NN model\n",
        "model.add(Conv1D(      # Adding a convolutional layer\n",
        "    filters,\n",
        "    kernel_size,\n",
        "    padding='valid',   # in this example the output is going to be slightly smaller\n",
        "    activation='relu',\n",
        "    strides=1,         # the shift\n",
        "    input_shape=(maxlen, embedding_dims))\n",
        "    )\n",
        "# Formulation: max (0, dot(filter, 3-gram))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWCmkuuZuLSa"
      },
      "outputs": [],
      "source": [
        "# Adding the max pooling\n",
        "# Alternatives\n",
        "# - GlobalMaxPooling1D() (the max for the entire filter's output)\n",
        "# - MaxPooling1D(n)  (the max for a specific area of n; default n=2)\n",
        "# - AvgPooling1D(n)\n",
        "\n",
        "model.add(GlobalMaxPooling1D())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDQdsfsZuLSa"
      },
      "source": [
        "**back to the slides to see what is pooling (and drop out)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGb5P7BluLSa"
      },
      "outputs": [],
      "source": [
        "# Adding dropout (20% of the data will be \"cancelled\")\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIeYhPXuuLSb"
      },
      "outputs": [],
      "source": [
        "# Adding the classification layer\n",
        "# sigmoid range: [0,1]\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988ejk35uLSb"
      },
      "source": [
        "**back to the slides**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWwi37nnuLSb"
      },
      "outputs": [],
      "source": [
        "# Compiling the CNN\n",
        "model.compile(loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za2KQuoyuLSb"
      },
      "outputs": [],
      "source": [
        "# Fitting (training) the model\n",
        "model.fit(x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTyXu8veuLSc"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "model_structure = model.to_json()\n",
        "with open(\"cnn_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)  # saves just the architecture\n",
        "model.save_weights(\"cnn_weights.h5\")  # saves the weights\n",
        "# You can run fit many times on the same model (it will continue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zhZfNxOuLSc"
      },
      "outputs": [],
      "source": [
        "# (Re)loading the model (not necessary here, but anyway)\n",
        "from keras.models import model_from_json\n",
        "with open(\"cnn_model.json\", \"r\") as json_file:\n",
        "    json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('cnn_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e6ggldtuLSc"
      },
      "outputs": [],
      "source": [
        "# Predicting a new instance\n",
        "\n",
        "# Notice we have both positive and negative words here\n",
        "sample_1 = \"\"\"I hate that the dismal weather had me down for so long,\n",
        "when will it break! Ugh, when does happiness return? The sun is\n",
        "blinding and the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
        "\n",
        "# The first value is a \"fake\" class (this is the expected input)\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
        "        embedding_dims))\n",
        "model.predict(test_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSCE6q5GuLSd"
      },
      "outputs": [],
      "source": [
        "# Get the class\n",
        "(model.predict(test_vec) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnSmL3eNuLSd"
      },
      "source": [
        "**End of the notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}