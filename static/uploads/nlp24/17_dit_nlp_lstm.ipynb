{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFXjkyseAkhD"
      },
      "source": [
        "##  DIT NLP lesson 2024\n",
        "\n",
        "# Long Short-Term Memory Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lselTFwAkhG"
      },
      "outputs": [],
      "source": [
        "# Importing the dependencies\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from random import shuffle\n",
        "from urllib import request\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3hA3vE2AkhH"
      },
      "outputs": [],
      "source": [
        "PATH_TO_CORPUS = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "CORPUS_FILE_NAME = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "PATH_TO_GOOGLENEWS_VECTORS =\"https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\"\n",
        "GOOGLE_VECTORS = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "CORPUS_PATH = \"aclImdb/train\"\n",
        "\n",
        "def download_file(url_to_file, path_to_file):\n",
        "  if os.path.isfile(path_to_file):\n",
        "    print(\"A local copy of the file exists already:\", path_to_file, \"\\nDoing nothing\")\n",
        "  else:\n",
        "    request.urlretrieve(url_to_file, path_to_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the embeddings\n",
        "\n",
        "download_file(PATH_TO_GOOGLENEWS_VECTORS, GOOGLE_VECTORS)\n",
        "\n",
        "# Downloading and untaring the corpus\n",
        "\n",
        "download_file(PATH_TO_CORPUS, CORPUS_FILE_NAME)\n",
        "with tarfile.open(CORPUS_FILE_NAME) as f:\n",
        "  f.extractall(path=\".\")"
      ],
      "metadata": {
        "id": "5GPOcEsPBCjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRCeTY0-AkhH"
      },
      "outputs": [],
      "source": [
        "# I am using the same preprocessing functions again\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    Load pos and neg examples from separate dirs then shuffle them\n",
        "    together.\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "    shuffle(dataset)\n",
        "    return dataset\n",
        "\n",
        "# Tokenizing and vectorizing all the instances\n",
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    for sample in dataset:\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(word_vectors[token])\n",
        "            except KeyError:\n",
        "                pass\n",
        "        vectorized_data.append(sample_vecs)\n",
        "    return vectorized_data\n",
        "\n",
        "# Not necessary in general; we apply it for comparison against\n",
        "# previous sessions\n",
        "def pad_trunc(data, maxlen):\n",
        "    \"\"\"\n",
        "    For a given dataset pad with zero vectors or truncate to maxlen\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "    # Create a vector of 0s the length of our word vectors\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            temp = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            temp = sample\n",
        "            # Append the appropriate number 0 vectors to the list\n",
        "            additional_elems = maxlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "\n",
        "    return new_data\n",
        "\n",
        "def collect_expected(dataset):\n",
        "    \"\"\"Extracting the expected output for all the instances\"\"\"\n",
        "    return [sample[0] for sample in dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OymwAHu0AkhI"
      },
      "outputs": [],
      "source": [
        "# Loading the embeddings\n",
        "word_vectors = KeyedVectors.load_word2vec_format(GOOGLE_VECTORS,\n",
        "    binary=True, limit=400000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHYRR6_ZAkhI"
      },
      "outputs": [],
      "source": [
        "# Data preparation\n",
        "dataset = pre_process_data(CORPUS_PATH)\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)\n",
        "split_point = int(len(vectorized_data) * .8)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jileuJ4EAkhI"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpHSJu5CAkhJ"
      },
      "outputs": [],
      "source": [
        "# Network parameters\n",
        "\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 300\n",
        "epochs = 2\n",
        "num_neurons = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ddppqi8AkhJ"
      },
      "outputs": [],
      "source": [
        "# Padding and truncating\n",
        "\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo4BKlXMAkhJ"
      },
      "outputs": [],
      "source": [
        "# Importing the dependencies\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imHWSNFGAkhJ"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Input([maxlen, embedding_dims]))\n",
        "model.add(\n",
        "    LSTM(num_neurons,\n",
        "         return_sequences=True,\n",
        "        )\n",
        "    )\n",
        "\n",
        "# This is what we had before\n",
        "# model.add(Bidirectional(SimpleRNN(\n",
        "#     num_neurons,\n",
        "#     return_sequences=True),\n",
        "#     )\n",
        "#  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXBkpgikAkhK"
      },
      "outputs": [],
      "source": [
        "# Adding a dropout, flattening, and classification layers\n",
        "model.add(Dropout(.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM6c3WwyAkhK"
      },
      "outputs": [],
      "source": [
        "# Compiling the network\n",
        "model.compile('rmsprop',\n",
        "              'binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_toruSGAkhK"
      },
      "source": [
        "That's **70,200** parameters in the LSTM (against **17,550** for the RNN)\n",
        "\n",
        "Back to the slides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSZY1HzlAkhK"
      },
      "outputs": [],
      "source": [
        "# Training the network\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iyY7q-NAkhK"
      },
      "outputs": [],
      "source": [
        "# Saving the network for future use\n",
        "model_structure = model.to_json()\n",
        "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)\n",
        "model.save_weights(\"lstm.weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyjM8KsGAkhK"
      },
      "outputs": [],
      "source": [
        "# Predicting\n",
        "sample_1 = \"\"\"I hate that the dismal weather had me down for so long, when\n",
        "will it break! Ugh, when does happiness return? The sun is blinding and\n",
        "the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
        "\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "test_vec = np.reshape(\n",
        "    test_vec_list,\n",
        "    (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsMCyCd5AkhK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}